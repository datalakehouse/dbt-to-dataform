{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil #directory controls\n",
    "import json\n",
    "import yaml\n",
    "import shutil\n",
    "import fnmatch\n",
    "import errno\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snowflake DBT to Dataform Converter\n",
    "\n",
    "### Edit variables below cell with the references of your project\n",
    "\n",
    "[Roadmap convertion spreadsheet](https://docs.google.com/spreadsheets/d/1q96HottHJaEC9vZ0NPrwLsoxVpnRE7fLGz5zI-y0mM0/edit#gid=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dbt source project path\n",
    "dbt_source_project_path = \"../../dbt/dbt_project_name/\"\n",
    "\n",
    "#dataform path to be created\n",
    "dataform_root_path = \"../_dataform_output\"\n",
    "\n",
    "#target schema on snowflake\n",
    "target_schema = 'TARGET_SCHEMA_NAME'\n",
    "\n",
    "#timestamp updated_at field to be used on snapshot convertion. \n",
    "#This field must represent the column name for the last updated date of the record\n",
    "dlh_timestamp_field = '\"MD_ELT_UPDATED_DTS\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating variables based on defined paths\n",
    "\n",
    "dbt_models_file_path = dbt_source_project_path+\"/models/\" \n",
    "dbt_snapshots_file_path = dbt_source_project_path+\"/snapshots/\" \n",
    "\n",
    "dataform_output_sources_path = dataform_root_path+\"/definitions/sources\"\n",
    "dataform_output_models_path = dataform_root_path+\"/definitions\"\n",
    "\n",
    "#JSON file of dataform connection to snowflake. https://docs.dataform.co/dataform-cli#create-a-credentials-file\n",
    "dataform_credentials_file_path = \"../.df-credentials.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find files on directories and subdirectories\n",
    "def find_files(directory, pattern):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for basename in files:\n",
    "            if fnmatch.fnmatch(basename, pattern):\n",
    "                filename = os.path.join(root, basename)\n",
    "                yield filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YAML to dictionary\n",
    "def read_yaml_file(filename):\n",
    "    with open(filename, 'r') as stream:\n",
    "        try:\n",
    "            yaml_dict = yaml.load(stream, Loader=yaml.FullLoader)\n",
    "            return yaml_dict\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function used to convert source (YML) to (SQLX) file\n",
    "def create_sqlx_json_source_file(models_file_path,dataform_output_sources_path):\n",
    "\n",
    "#creating directory\n",
    "    isExist = os.path.exists(dataform_output_sources_path)\n",
    "    if not isExist:\n",
    "        os.makedirs(dataform_output_sources_path)\n",
    "\n",
    "#iterate through each yaml file that contains sources\n",
    "    for file in models_file_path:\n",
    "        #reading all YAML files on source directory\n",
    "        dic = read_yaml_file(file)\n",
    "        #getting only sources entity from yaml file\n",
    "        if 'sources' in dic.keys():\n",
    "\n",
    "            sources = dic[\"sources\"]\n",
    "            \n",
    "            for source in sources:\n",
    "                #declaring variables for each element of dictionary that will be used. tables = list, schema and database = str\n",
    "                tables = source[\"tables\"]            \n",
    "                database = source[\"database\"]\n",
    "                schema = source[\"schema\"]\n",
    "\n",
    "                #iterating through all tables for each schema\n",
    "                for tables in tables:\n",
    "                        #getting tables from list\n",
    "                        tables= tables['name']                                \n",
    "                        #creating json .sqlx structure\n",
    "                        jmodel = \"{\\n\"\n",
    "                        jmodel += f'\"type\": \"declaration\",\\n'                \n",
    "                        jmodel += f'\"database\": \"{database}\",\\n'\n",
    "                        jmodel += f'\"schema\": \"{schema}\",\\n'\n",
    "                        jmodel += f'\"name\": \"{tables}\"'\n",
    "                        jmodel += \"\\n}\" #close out JSON file EOF\n",
    "\n",
    "                        #parsing json and replacing double quotes for single quotes on keys\n",
    "                        parsed = json.dumps(json.loads(jmodel), sort_keys=False, indent=2).replace('\"type\":','type:').replace('\"database\":','database:').replace('\"schema\":','schema:').replace('\"name\":','name:')\n",
    "\n",
    "                        #creating .sqlx files\n",
    "                        #write out to file in the appropriate location defined in variables\n",
    "                        with open(f'{dataform_output_sources_path}/{schema}_{tables}.sqlx', \"w\") as jmodel_file:                            \n",
    "                            print('Generated file: '+f'{dataform_output_sources_path}/{schema}_{tables}.sqlx')\n",
    "                            jmodel_file.write(\"config \"+parsed)\n",
    "                            jmodel_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function used to convert DBT SQL model files to SQLX model files on dataform\n",
    "\n",
    "def create_sqlx_models_files(model_files_path,dataform_output_models_path):\n",
    "    #iterate on each SQL file on DBT directory and getting variables\n",
    "    for filename in model_files_path:  \n",
    "        single_file_name = os.path.basename(filename)\n",
    "        destination_path = os.path.dirname(filename)\n",
    "        destination_path = dataform_output_models_path+(destination_path.split('models')[1])+'/'\n",
    "        destination_full_path = (destination_path+(os.path.basename(filename)).replace(\"sql\",\"sqlx\"))\n",
    "        \n",
    "     #######copying files to new directory##############   \n",
    "        try:\n",
    "            #copy files if directory already exists            \n",
    "            shutil.copyfile(filename,destination_full_path)\n",
    "        except IOError as e:\n",
    "            # ENOENT(2): file does not exist, raised also on missing dest parent dir\n",
    "            if e.errno != errno.ENOENT:\n",
    "                raise\n",
    "            # create directory if not exists           \n",
    "            os.makedirs(destination_path)\n",
    "            # copy SQL files to new directory          \n",
    "            shutil.copyfile(filename,destination_full_path)\n",
    "    #######end copying files to new directory##############\n",
    "\n",
    "    \n",
    "    #######reading file and replacing model syntax differences########\n",
    "        # Read in the file\n",
    "        with open(destination_full_path, 'r') as file :\n",
    "            filedata = file.read()\n",
    "            #getting header data to be replaced\n",
    "            header=filedata[filedata.find(\"{{\"):filedata.find(\"}}\")+2]\n",
    "            header_old=header\n",
    "            #dictionary to replace simple syntax elements\n",
    "            header_replace_dict = {\"{{\":\"\", \"}}\":\"}\", \"materialized\":\"type\",\"=\":\":\",\"(\":\"{\",\")\":\"\",\"'\":'\"',\"unique_key\":'uniqueKey'}\n",
    "\n",
    "            #iterate through dictionary keys\n",
    "            for key in header_replace_dict.keys():\n",
    "                #replacing all headers based on dict mapping to 'header' variable\n",
    "                header = header.replace(key, header_replace_dict[key])         \n",
    "\n",
    "\n",
    "        #writing file with replaced header and models references\n",
    "        with open(destination_full_path, 'w') as file:\n",
    "            #writting on file the replaced header based on dictionary \n",
    "            new_file = filedata.replace(header_old,header)\n",
    "            #converting transient tables to snowflake specific block\n",
    "            new_file = re.sub(r'(transient)[ ]{0,9}[\\:][ ]{0,9}(true|false|True|False|TRUE|FALSE)',r'snowflake: { \\n     \\1: \\2 \\n}',new_file)\n",
    "            #converting disabled models syntax\n",
    "            new_file = re.sub(r'(enabled)[ ]{0,9}[\\:][ ]{0,9}(true|True|TRUE)','',new_file)\n",
    "            new_file = re.sub(r'(enabled)[ ]{0,9}[\\:][ ]{0,9}(false|False|FALSE)','disabled: true',new_file)\n",
    "            #removing configurations that does not exists on dataform\n",
    "            new_file = re.sub(r'(pre_hook|post_hook|alias|meta|persist_docs|merge_update_columns|on_schema_change)[ ]{0,9}[\\:][ ]{0,9}.*[,}\\\")]','',new_file)\n",
    "            #changing {{ref to ${ref\n",
    "            new_file = re.sub(r\"[{][{][ ]{0,6}(ref|REF|Ref|SOURCE|source|Source)[ ]{0,9}[(][ ]{0,9}[']?\",'${ref(\"', new_file)     \n",
    "            #including just 1 space before config header\n",
    "            new_file = re.sub(r'(config)[ ]{0,40}[{]{0,40}','config {',new_file)\n",
    "            #closing ) with \") after sources and refs\n",
    "            new_file = re.sub(r\"(['][)][ ]{0,9}[}])\",'\")',new_file)\n",
    "            #removing alone commmas on start of config block\n",
    "            new_file = re.sub(r\"({)\\n[ ]{0,100}[,]\",'{',new_file)\n",
    "            #changing syntax reference to macros from {{ to ${\n",
    "            new_file = re.sub(r\"(?<!\\')({{)[ ]{0,20}[.]{0,20}\",'${',new_file)\n",
    "            new_file = re.sub(r\"([\\$].+)[ ]{0,9}(?<=\\()(')\",r'\\1\"',new_file)\n",
    "            #changing ' to \" on sources references\n",
    "            new_file = re.sub(r\"([\\$].+)((?<=[0-9A-Za-z])[ ]{0,9}[_'][.]{0,10}[ ]{0,20}[,][.]{0,20}[ ]{0,20}[.]{0,20}['])\",r'\\1\",\"',new_file)\n",
    "            #changing invocation_id to snowflake uuid_string\n",
    "            new_file = re.sub(r\"('{{invocation_id}}'|{{invocation_id}})\",'uuid_string()',new_file)\n",
    "            #changing incremental loads macro\n",
    "            new_file = re.sub(r\"({[ ]{0,10}%[ ]{0,10}if[ ]{0,10}is_incremental.*)((\\s*)(.*)\\s*)({%[ ]{0,10}endif..*})\",r'${ when(incremental(), \\n`\\4`) }',new_file)\n",
    "            new_file = re.sub(r\"((?:\\${[ ]{0,10}when.*(\\s.*)))(\\${this\\}\\})\",r'\\1${self()}',new_file)\n",
    "            \n",
    "            \n",
    "\n",
    "            try:\n",
    "                file.write(new_file)                \n",
    "                print('Generated file: '+destination_full_path)\n",
    "            except IOError as e:\n",
    "                print (\"I/O error({0}): {1}\".format(e.errno, e.strerror))\n",
    "    #######end reading file and replacing model syntax differences########\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sqlx_snapshot_files(snapshots_file_path,dataform_output_models_path):\n",
    "    for filename in snapshots_file_path:  \n",
    "        single_file_name = os.path.basename(filename)\n",
    "        destination_path = os.path.dirname(filename)\n",
    "        destination_path = dataform_output_models_path+'/snapshots'+(destination_path.split('snapshots')[1])+'/'\n",
    "        destination_full_path = (destination_path+(os.path.basename(filename)).replace(\".sql\",\".js\"))\n",
    "\n",
    "     #######copying files to new directory##############   \n",
    "        try:\n",
    "            #copy files if directory already exists\n",
    "\n",
    "            shutil.copyfile(filename,destination_full_path)\n",
    "        except IOError as e:\n",
    "            # ENOENT(2): file does not exist, raised also on missing dest parent dir\n",
    "            if e.errno != errno.ENOENT:\n",
    "                raise\n",
    "            # create directory if not exists\n",
    "            os.makedirs(destination_path)\n",
    "            # copy source files to new directory\n",
    "            shutil.copyfile(filename,destination_full_path)\n",
    "    #######end copying files to new directory##############\n",
    "\n",
    "        #read copied files\n",
    "        with open(destination_full_path, 'r') as file :\n",
    "            filedata = file.read()   \n",
    "            #getting the name of the snapshot source table\n",
    "            table = re.search(r\"({{)[ ]{0,9}(ref).*((?<=').*(?='))\",filedata).group(3)\n",
    "            #getting the name of the file and removing extension\n",
    "            file_name = single_file_name.replace('.sql','')\n",
    "            #removing macro references \n",
    "            filedata = re.sub(r'({%)[ ]{0,10}(endsnapshot|snapshot).*(})','',filedata)\n",
    "            #if exists check cols (not supported to data form), replace for timestamp field\n",
    "            filedata = re.sub(r\"(check_cols.*[\\]]{1,1}|check_cols.*[\\\"]{1,1})\",'timestamp: '+dlh_timestamp_field,filedata)\n",
    "            #getting header block\n",
    "            filedata=filedata[filedata.find(\"{\"):filedata.find(\"}\")+2]\n",
    "            #replacing fields on header block\n",
    "            filedata = filedata.replace(')', 'source: {\\n schema: \"{{schema}}\",\\n name: \"{{table}}\",\\n}, \\n});')\n",
    "            filedata = filedata.replace('{{schema}}',target_schema).replace('{{table}}',table)\n",
    "            #create a dict with some syntax differences\n",
    "            replace_dict = {\"updated_at\":\"timestamp\",\"unique_key\":\"  uniqueKey\", \"config\":'scd(\"{{source_data_scd}}\", ',\"'\":'\"',\"{{\":\"\",\"}}\":\"\",\"=\":\": \"}\n",
    "            \n",
    "            #iterate through dictionary keys\n",
    "            for key in replace_dict.keys():\n",
    "                #replacing all headers\n",
    "                filedata = filedata.replace(key, replace_dict[key]).replace('{{source_data_scd}}',file_name)\n",
    "                filedata = re.sub(r'(scd\\(.*)(\\()',r'\\1{',filedata)\n",
    "\n",
    "\n",
    "        #writing file with replaced header and models references\n",
    "        with open(destination_full_path, 'w') as file:\n",
    "            #replacing files content\n",
    "            new_file = filedata\n",
    "            new_file = re.sub(r'(strategy)[ ]{0,9}[\\:][ ]{0,9}.*(,)','',new_file)   \n",
    "            #removing unsupported configurations\n",
    "            new_file = re.sub(r'(target_database|target_schema|strategy|invalidate_hard_deletes|check_cols)[ ]{0,9}[\\:][ ]{0,9}.*[,}\\\"\\')]','',new_file)\n",
    "            new_file = re.sub(r\"[ ]{2,999}\",'',new_file)\n",
    "            new_file = re.sub(r\"^\\s*$\",'',new_file,re.MULTILINE)\n",
    "            new_file = re.sub(r'\\n\\s*\\n','\\n',new_file,re.MULTILINE)            \n",
    "            #insert space to ident\n",
    "            new_file = new_file.replace('uniqueKey:','  uniqueKey:').replace('timestamp:','  timestamp:').replace('source:','  source:').replace('schema:','   schema:').replace('name:','   name:').replace('},','  },')\n",
    "            #create .JS snapshot file\n",
    "            try:\n",
    "                file.write('const scd = require(\"dataform-scd\");\\n\\n')\n",
    "                file.write(new_file)\n",
    "                print('Generated file: '+destination_full_path)\n",
    "            except IOError as e:\n",
    "                print (\"I/O error({0}): {1}\".format(e.errno, e.strerror))\n",
    "                \n",
    "    #######end reading file and replacing model syntax differences########\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataform_install_configuration(dataform_root_path,dataform_credentials_file_path):\n",
    "    \n",
    "    try:\n",
    "        shutil.rmtree(dataform_root_path)\n",
    "    except OSError as err:\n",
    "        print(err)\n",
    "\n",
    "    #initiating dataform new project\n",
    "    os.system(\"dataform init snowflake \"+dataform_root_path)\n",
    "    #copying snowflake credentials file\n",
    "    shutil.copy(dataform_credentials_file_path,dataform_root_path)\n",
    "    \n",
    "    packages_file = dataform_root_path+ '/' +'package.json'\n",
    "        \n",
    "    with open(packages_file, 'r') as file:\n",
    "        packages_file_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataform_assertions_documentation(schema_files_path,dataform_output_models_path):\n",
    "\n",
    "    for file in schema_files_path:\n",
    "        #reading all YAML files on source directory\n",
    "        dic = read_yaml_file(file)\n",
    "        #getting only sources entity from yaml file\n",
    "        if 'models' in dic.keys():\n",
    "\n",
    "            models_dir = dic[\"models\"]\n",
    "            dictionary={}\n",
    "            #print(models_dir)\n",
    "            unique_list=[]\n",
    "            not_null_list=[]\n",
    "            not_null = {} \n",
    "            table_descriptions = {} \n",
    "            description={}\n",
    "            unique = {}  \n",
    "            tables_list=[]\n",
    "            description_list=[]\n",
    "            table_description_list=[]\n",
    "            jmodel_tables_description=''\n",
    "            jmodel_description=''\n",
    "            jmodel=''\n",
    "            for model in models_dir:\n",
    "                #declaring variables for each element of dictionary that will be used. tables = list, schema and database = str        \n",
    "                #print(model[\"name\"])\n",
    "                if 'columns' in model:\n",
    "                    columns = (model[\"columns\"]) \n",
    "                if 'name' in model:\n",
    "                    tables = (model[\"name\"])\n",
    "\n",
    "\n",
    "                tables_list.append(tables)\n",
    "                #getting tables descriptions\n",
    "                if 'description' in model: \n",
    "                    descriptions = (model[\"description\"])\n",
    "                    table_descriptions = {descriptions:tables}\n",
    "                    table_description_list.append(table_descriptions.copy())\n",
    "\n",
    "\n",
    "                #getting column tests and descriptions\n",
    "                for column in columns:\n",
    "                    tests= (column.get('tests'))\n",
    "                    column_get = column.get('name')           \n",
    "                    descriptions= (column.get('description'))\n",
    "\n",
    "                    #print(tests)\n",
    "\n",
    "                    if 'unique' in tests:\n",
    "                        unique = {column_get:tables}                                                        \n",
    "                        unique_list.append(unique.copy())\n",
    "\n",
    "\n",
    "                    if 'not_null' in tests:\n",
    "                        not_null = {column_get:tables}\n",
    "                        not_null_list.append(not_null.copy())\n",
    "\n",
    "\n",
    "                    if 'description'in column:\n",
    "                        description = {'|    '+column_get+': '+\"'\"+descriptions+\"'\":tables}\n",
    "                        description_list.append(description.copy())\n",
    "\n",
    "\n",
    "            #print(not_null_list)\n",
    "            \n",
    "            keyfunc = lambda d: next(iter(d.values()))   \n",
    "\n",
    "            print('Found table description' +str(table_description_list)+'\\n')\n",
    "            print('Found uniqueKey test' +str(unique_list)+'\\n')\n",
    "            print('Found Not_Null test' +str(not_null_list)+'\\n')\n",
    "            print('Found column description' +str(description_list)+'\\n')\n",
    "            \n",
    "            #create dictionary for data tests and tables/columns descriptions\n",
    "            not_null_dict={k: [x for d in g for x in d] \n",
    "                for k, g in itertools.groupby(sorted(not_null_list, key=keyfunc), key=keyfunc)}\n",
    "\n",
    "            unique_dict={k: [x for d in g for x in d] \n",
    "                for k, g in itertools.groupby(sorted(unique_list, key=keyfunc), key=keyfunc)}\n",
    "\n",
    "            description_dict={k: [x for d in g for x in d] \n",
    "                for k, g in itertools.groupby(sorted(description_list, key=keyfunc), key=keyfunc)}\n",
    "\n",
    "            table_description_dict={k: [x for d in g for x in d] \n",
    "                    for k, g in itertools.groupby(sorted(table_description_list, key=keyfunc), key=keyfunc)}\n",
    "\n",
    "\n",
    "            # list of all .sql files in a directory\n",
    "            dataform_files = find_files(dataform_output_models_path, '*.sqlx')\n",
    "\n",
    "\n",
    "            files_list =[]\n",
    "            file_name_list =[]\n",
    "            \n",
    "            #getting all model files on dataform path\n",
    "            for file in dataform_files:\n",
    "                #only tables that has descriptions or tests\n",
    "                if os.path.basename(file).replace('.sqlx','') in tables_list:\n",
    "                    file_name = os.path.basename(file)\n",
    "                    destination_path = os.path.dirname(file)\n",
    "                    full_path = destination_path+'/'+file_name     \n",
    "                    files_list.append(file_name.replace('.sqlx',''))\n",
    "\n",
    "\n",
    "                    #read files\n",
    "                    with open(full_path, 'r') as file :\n",
    "\n",
    "                        file_name = file_name.replace('.sqlx','')\n",
    "                        #add description to table (setting variable)\n",
    "                        if file_name in table_description_dict.keys():\n",
    "                            jmodel_tables_description='  description: '+str(table_description_dict[file_name]).replace(\"'\",'\"').replace('[','').replace(']','')+',\\n'\n",
    "\n",
    "                        #add assertions { on variable\n",
    "                        filedata = file.read()\n",
    "                        if file_name in unique_dict.keys() or file_name in unique_dict.keys():\n",
    "                            jmodel='  assertions: {\\n'\n",
    "\n",
    "\n",
    "                            #when exists uniqueKey test on dbt, create on dataform\n",
    "                            if file_name in unique_dict.keys():                    \n",
    "                                jmodel+='    uniqueKey: '+str(unique_dict[file_name]).replace(\"'\",'\"')+',\\n'\n",
    "\n",
    "                            #when exists NotNull test on dbt, create on dataform\n",
    "                            if file_name in not_null_dict.keys():\n",
    "                                jmodel+='    nonNull: '+str(not_null_dict[file_name]).replace(\"'\",'\"')+'\\n'\n",
    "\n",
    "\n",
    "                            jmodel+='\\n  }\\n}'\n",
    "\n",
    "                        #add column description on dataform\n",
    "                        if file_name in description_dict.keys():         \n",
    "                            jmodel_description = '  columns: {\\n'\n",
    "                            jmodel_description += str(description_dict[file_name]).replace('\"','').replace(\"'\",'\"').replace('[','').replace(']','').replace('|','\\n')\n",
    "                            jmodel_description += '\\n  }\\n'\n",
    "\n",
    "\n",
    "                        #setting variable with all tests and description to be added on model\n",
    "                        header=filedata[filedata.find(\"{\"):filedata.find(\"}\")+2]                \n",
    "                        header_old=header\n",
    "                        header = header.replace('}','')+jmodel_tables_description+jmodel_description+jmodel\n",
    "\n",
    "        \n",
    "                        with open(f'{full_path}', \"w\") as jmodel_file:\n",
    "                                #replace old header to new header with assertions and column or table documentations\n",
    "                                jmodel_file.write(filedata.replace(header_old,header))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all .sql files in a directory\n",
    "model_files = find_files(dbt_models_file_path, '*.sql')\n",
    "# list of all .sql files in a directory\n",
    "snapshot_files = find_files(dbt_snapshots_file_path, '*.sql')\n",
    "# list of test and documentation files\n",
    "source_files = find_files(dbt_models_file_path, '*.yml')\n",
    "# list of test and documentation files\n",
    "schema_files = find_files(dbt_models_file_path, '*.yml')\n",
    "# dataform output files\n",
    "dataform_output_models_path = \"../_dataform_output/definitions\"\n",
    "\n",
    "def dbt_dataform_converter(dataform_root_path,model_files,snapshot_files,source_files,schema_files,dataform_output_models_path):\n",
    "    \n",
    "    \n",
    "    #installing dataform\n",
    "    dataform_install_configuration(dataform_root_path,dataform_credentials_file_path)    \n",
    "    #creating source\n",
    "    create_sqlx_json_source_file(source_files,dataform_output_sources_path)\n",
    "    #creating models\n",
    "    create_sqlx_models_files(model_files,dataform_output_models_path)\n",
    "    #creating snapshot files\n",
    "    create_sqlx_snapshot_files(snapshot_files,dataform_output_models_path)\n",
    "    #addind test to models\n",
    "    dataform_assertions_documentation(schema_files,dataform_output_models_path)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing project files...\n",
      "\n",
      "\u001b[32mDirectories successfully created:\u001b[0m\n",
      "  /Users/guilhermealcantara/OneDrive/brf consulting/dbt - dataform converter/_dataform_output\n",
      "  /Users/guilhermealcantara/OneDrive/brf consulting/dbt - dataform converter/_dataform_output/definitions\n",
      "  /Users/guilhermealcantara/OneDrive/brf consulting/dbt - dataform converter/_dataform_output/includes\n",
      "\u001b[32mFiles successfully written:\u001b[0m\n",
      "  /Users/guilhermealcantara/OneDrive/brf consulting/dbt - dataform converter/_dataform_output/dataform.json\n",
      "  /Users/guilhermealcantara/OneDrive/brf consulting/dbt - dataform converter/_dataform_output/package.json\n",
      "  /Users/guilhermealcantara/OneDrive/brf consulting/dbt - dataform converter/_dataform_output/.gitignore\n",
      "\u001b[32mNPM packages successfully installed.\u001b[0m\n",
      "Generated file: ../_dataform_output/definitions/sources/DEMO_SQUARE_ALT13_CUSTOMER.sqlx\n",
      "Generated file: ../_dataform_output/definitions/sources/DEMO_SQUARE_ALT13_ORDER_DISCOUNT.sqlx\n",
      "Generated file: ../_dataform_output/definitions/sources/DEMO_SQUARE_ALT13_ORDER_LINE_ITEM.sqlx\n",
      "Generated file: ../_dataform_output/definitions/sources/DEMO_SQUARE_ALT13_ORDER.sqlx\n",
      "Generated file: ../_dataform_output/definitions/sources/DEMO_SQUARE_ALT13_PAYMENT.sqlx\n",
      "Generated file: ../_dataform_output/definitions/sources/DEMO_SQUARE_ALT13_CATALOG_ITEM.sqlx\n",
      "Generated file: ../_dataform_output/definitions/sources/DEMO_SQUARE_ALT13_CATALOG_ITEM_VARIATION.sqlx\n",
      "Generated file: ../_dataform_output/definitions/sources/DEMO_SQUARE_ALT13_CATALOG_CATEGORY.sqlx\n",
      "Generated file: ../_dataform_output/definitions/sources/DEMO_SQUARE_ALT13_CATALOG_MODIFIER.sqlx\n",
      "Generated file: ../_dataform_output/definitions/sources/DEMO_SQUARE_ALT13_CATALOG_TAX.sqlx\n",
      "Generated file: ../_dataform_output/definitions/sources/DEMO_SQUARE_ALT13_CATALOG_DISCOUNT.sqlx\n",
      "Generated file: ../_dataform_output/definitions/sources/DEMO_SQUARE_ALT13_ORDER_LINE_ITEM_MODIFIER.sqlx\n",
      "Generated file: ../_dataform_output/definitions/sources/DEMO_SQUARE_ALT13_LOCATION.sqlx\n",
      "Generated file: ../_dataform_output/definitions/staging/CUSTOMERS/V_CUSTOMER_STG.sqlx\n",
      "Generated file: ../_dataform_output/definitions/staging/LOCATION/V_MERCHANT_LOCATION_STG.sqlx\n",
      "Generated file: ../_dataform_output/definitions/staging/PAYMENT/V_PAYMENT_STG.sqlx\n",
      "Generated file: ../_dataform_output/definitions/staging/CATALOG/V_CATALOG_ITEM_MODIFIER_STG.sqlx\n",
      "Generated file: ../_dataform_output/definitions/staging/CATALOG/V_CATALOG_TAX_STG.sqlx\n",
      "Generated file: ../_dataform_output/definitions/staging/CATALOG/V_CATALOG_CATEGORY_STG.sqlx\n",
      "Generated file: ../_dataform_output/definitions/staging/CATALOG/V_CATALOG_DISCOUNT_STG.sqlx\n",
      "Generated file: ../_dataform_output/definitions/staging/CATALOG/V_CATALOG_ITEM_VARIATION_STG.sqlx\n",
      "Generated file: ../_dataform_output/definitions/staging/DATE/V_DATE_STG.sqlx\n",
      "Generated file: ../_dataform_output/definitions/staging/ORDERS/V_ORDER_LINE_ITEM_STG.sqlx\n",
      "Generated file: ../_dataform_output/definitions/staging/ORDERS/V_ORDER_HEADER_STG.sqlx\n",
      "Generated file: ../_dataform_output/definitions/staging/ORDERS/V_ORDER_LINE_ITEM_MODIFIER_STG.sqlx\n",
      "Generated file: ../_dataform_output/definitions/staging/CURRENCY/V_CURRENCY_STG.sqlx\n",
      "Generated file: ../_dataform_output/definitions/master/W_ORDERS_F.sqlx\n",
      "Generated file: ../_dataform_output/definitions/master/W_DATE_D.sqlx\n",
      "Generated file: ../_dataform_output/definitions/master/W_MERCHANT_LOCATION_D.sqlx\n",
      "Generated file: ../_dataform_output/definitions/master/W_PAYMENTS_F.sqlx\n",
      "Generated file: ../_dataform_output/definitions/master/W_CUSTOMERS_D.sqlx\n",
      "Generated file: ../_dataform_output/definitions/master/W_CURRENCY_D.sqlx\n",
      "Generated file: ../_dataform_output/definitions/master/W_CATALOG_ITEM_D.sqlx\n",
      "Generated file: ../_dataform_output/definitions/snapshots/W_CUSTOMERS_SNAPSHOT_D.js\n",
      "Found table description[{'This is the customers dimension table': 'W_CUSTOMERS_D'}, {'This is the catalog item dimension table': 'W_CATALOG_ITEM_D'}, {'This is the currency dimension table': 'W_CURRENCY_D'}, {'This is the merchant location dimension table': 'W_MERCHANT_LOCATION_D'}]\n",
      "\n",
      "Found uniqueKey test[{'K_POS_CUSTOMER_DLHK': 'W_CUSTOMERS_D'}, {'K_POS_CATALOG_OBJECT_HASH_DLHK': 'W_CATALOG_ITEM_D'}, {'K_CURRENCY_DLHK': 'W_CURRENCY_D'}, {'K_MERCH_LOC_DLHK': 'W_MERCHANT_LOCATION_D'}]\n",
      "\n",
      "Found Not_Null test[{'K_POS_CUSTOMER_DLHK': 'W_CUSTOMERS_D'}, {'TESTE': 'W_CUSTOMERS_D'}, {'K_POS_CATALOG_OBJECT_HASH_DLHK': 'W_CATALOG_ITEM_D'}, {'K_CURRENCY_DLHK': 'W_CURRENCY_D'}, {'K_MERCH_LOC_DLHK': 'W_MERCHANT_LOCATION_D'}]\n",
      "\n",
      "Found column description[{\"|    K_POS_CUSTOMER_DLHK: 'DataLakeHouse key of the customer'\": 'W_CUSTOMERS_D'}, {\"|    TESTE: 'DataLakeHouse test'\": 'W_CUSTOMERS_D'}, {\"|    K_POS_CATALOG_OBJECT_HASH_DLHK: 'DataLakeHouse key of the item'\": 'W_CATALOG_ITEM_D'}, {\"|    K_CURRENCY_DLHK: 'DataLakeHouse key of the currency'\": 'W_CURRENCY_D'}, {\"|    K_MERCH_LOC_DLHK: 'DataLakeHouse key of the currency'\": 'W_MERCHANT_LOCATION_D'}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dbt_dataform_converter(dataform_root_path,model_files,snapshot_files,source_files,schema_files,dataform_output_models_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
