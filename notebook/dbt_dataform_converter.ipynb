{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil #directory controls\n",
    "import json\n",
    "import yaml\n",
    "import shutil\n",
    "import fnmatch\n",
    "import errno\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snowflake DBT to Dataform Converter\n",
    "\n",
    "### Edit variables on the cell below with the references of your project\n",
    "\n",
    "[Instructions](https://github.com/datalakehouse/dbt-to-dataform/blob/DLHX-789-dbt-to-dataform/README.md)\n",
    "\n",
    "[Roadmap convertion spreadsheet](https://docs.google.com/spreadsheets/d/1q96HottHJaEC9vZ0NPrwLsoxVpnRE7fLGz5zI-y0mM0/edit#gid=0).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dbt source project path\n",
    "dbt_source_project_path = \"../../dbt/dlh-bill-dot-com-analytics-dbt\"\n",
    "#dbt_source_project_path = \"../../dbt/stripe_dbt\"\n",
    "\n",
    "#dataform path to be created\n",
    "dataform_root_path = \"../../dataform/test_bill_dot_com\"\n",
    "#dataform_root_path = \"../../dataform/test_stripe\"\n",
    "\n",
    "#target schema on snowflake\n",
    "target_schema = 'DATALAKEHOUSE_DATAFORM_BILL_DOT_COM'\n",
    "\n",
    "#define the type of conversion (js or sqlx)\n",
    "conversion_type = 'js' #or sqlx\n",
    "\n",
    "#timestamp updated_at field to be used on snapshot convertion. \n",
    "#This field must represent the column name for the last updated date of the record\n",
    "dlh_timestamp_field = '\"MD_ELT_UPDATED_DTS\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating variables based on defined paths\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "dbt_models_file_path = dbt_source_project_path+\"/models/\" \n",
    "dbt_snapshots_file_path = dbt_source_project_path+\"/snapshots/\" \n",
    "dbt_macros_file_path = dbt_source_project_path+\"/macros\"\n",
    "\n",
    "dataform_output_sources_path = dataform_root_path+\"/definitions/sources\"\n",
    "dataform_output_models_path = dataform_root_path+\"/definitions\"\n",
    "dataform_output_includes_path = dataform_root_path+\"/includes\"\n",
    "\n",
    "\n",
    "#JSON file of dataform connection to snowflake. https://docs.dataform.co/dataform-cli#create-a-credentials-file\n",
    "dataform_credentials_file_path = \"../.df-credentials.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find files on directories and subdirectories\n",
    "def find_files(directory, pattern):\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for basename in files:\n",
    "                if fnmatch.fnmatch(basename, pattern):\n",
    "                    filename = os.path.join(root, basename)                    \n",
    "                    yield filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YAML to dictionary\n",
    "def read_yaml_file(filename):\n",
    "    with open(filename, 'r') as stream:\n",
    "        try:\n",
    "            yaml_dict = yaml.load(stream, Loader=yaml.FullLoader)\n",
    "            return yaml_dict\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function used to convert source (YML) to (JS) file\n",
    "def create_js_source_file(dbt_models_file_path,dataform_output_sources_path,conversion_type):\n",
    "\n",
    "    #check if source directory exists        \n",
    "    isExistSource = os.path.exists(dbt_models_file_path)\n",
    "\n",
    "\n",
    "\n",
    "    if not isExistSource:\n",
    "        print('Directory does not exists:' + dbt_models_file_path)\n",
    "    else:\n",
    "        source_files = find_files(dbt_models_file_path, '*.yml')\n",
    "    #creating directory        \n",
    "        isExist = os.path.exists(dataform_output_sources_path)\n",
    "\n",
    "        if not isExist and conversion_type == 'sqlx':\n",
    "            os.makedirs(dataform_output_sources_path)\n",
    "\n",
    "\n",
    "        footer = '  return {\\n'\n",
    "        js_parsed=''\n",
    "\n",
    "    #iterate through each yaml file that contains sources\n",
    "        for file in source_files:\n",
    "            #reading all YAML files on source directory\n",
    "            dic = read_yaml_file(file)\n",
    "\n",
    "            #getting only sources entity from yaml file\n",
    "\n",
    "            if 'sources' in dic.keys():\n",
    "                sources = dic[\"sources\"]\n",
    "\n",
    "                for source in sources:                    \n",
    "\n",
    "                    #declaring variables for each element of dictionary that will be used. tables = list, schema and database = str\n",
    "                    table = source[\"tables\"]            \n",
    "                    database = source[\"database\"]\n",
    "                    schema = source[\"schema\"]\n",
    "\n",
    "\n",
    "\n",
    "                    for tables in table:\n",
    "                        #getting tables from list\n",
    "                        tables= tables['name']\n",
    "\n",
    "                        database = re.sub(r\"({{[ ]{0,10}var[ ]{0,9}\\([ ]{0,9}')(.*)('[ ]{0,9},[ ]{0,9}')(.*)\",r'\\2',database)\n",
    "                        schema = re.sub(r\"({{[ ]{0,10}var[ ]{0,9}\\([ ]{0,9}')(.*)('[ ]{0,9},[ ]{0,9}')(.*)\",r'\\2',schema)\n",
    "\n",
    "                        #creating json .sqlx structure\n",
    "                        if conversion_type == 'sqlx':\n",
    "                            jmodel = \"declare({\\n\"\n",
    "                        else:\n",
    "                            jmodel = f'const {tables} = declare('+'{\\n'\n",
    "                            footer += f'    {tables}'+',\\n'\n",
    "                        #jmodel += f'\"type\": \"declaration\",\\n'                \n",
    "                        jmodel += f'  \"database\": {database},\\n'\n",
    "                        jmodel += f'  \"schema\": {schema},\\n'\n",
    "                        jmodel += f'  \"name\": \"{tables}\"'\n",
    "                        jmodel += \"\\n});\\n\" #close out JSON file EOF\n",
    "                     #parsing json and replacing double quotes for single quotes on keys\n",
    "                        parsed = jmodel.replace('\"database\":','database:').replace('\"schema\":','schema:').replace('\"name\":','name:')\n",
    "                        parsed = re.sub(r'(\\\"{{)[ ]{0,9}(var)([\\(][\\'](.*)(\\'\\)}}\\\"))', r'constants.\\4', parsed)\n",
    "                        #creating .sqlx files\n",
    "                        #write out to file in the appropriate location defined in variables\n",
    "\n",
    "\n",
    "                        if conversion_type == 'sqlx':\n",
    "\n",
    "                            with open(f'{dataform_output_sources_path}/sources.js', \"a\") as jmodel_file:                            \n",
    "                                print(f'Creating source : {database}_{schema}_{tables}')\n",
    "                                jmodel_file.write(parsed)\n",
    "                                jmodel_file.close()\n",
    "\n",
    "                        else:                        \n",
    "                            js_parsed += parsed.replace('constants.','')\n",
    "\n",
    "                if conversion_type == 'js':\n",
    "                    return(js_parsed+footer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function used to convert DBT SQL model files to SQLX model files on dataform\n",
    "\n",
    "def create_sqlx_models_files(dbt_models_file_path,dataform_output_models_path):\n",
    "          \n",
    "    #check if source directory exists        \n",
    "    isExistSource = os.path.exists(dbt_models_file_path)\n",
    "    if not isExistSource:\n",
    "        print('Directory does not exists:' + dbt_models_file_path)\n",
    "    else:\n",
    "        model_files_path = find_files(dbt_models_file_path, '*.sql')\n",
    "        \n",
    "                                \n",
    "        with open(dataform_output_models_path+'/H_INVOCATION_ID.sqlx', 'w+') as f:\n",
    "            f.write('config {\\n  type: \"table\" \\n} \\n SELECT uuid_string() as invocation_id from dual')\n",
    "\n",
    "        #iterate on each SQL file on DBT directory and getting variables\n",
    "        for filename in model_files_path:  \n",
    "            single_file_name = os.path.basename(filename)\n",
    "            destination_path = os.path.dirname(filename)\n",
    "            destination_path = dataform_output_models_path+(destination_path.split('models')[1])+'/'\n",
    "            destination_full_path = (destination_path+(os.path.basename(filename)).replace(\"sql\",\"sqlx\"))\n",
    "\n",
    "         #######copying files to new directory##############   \n",
    "            try:\n",
    "                #copy files if directory already exists            \n",
    "                shutil.copyfile(filename,destination_full_path)\n",
    "            except IOError as e:\n",
    "                # ENOENT(2): file does not exist, raised also on missing dest parent dir\n",
    "                if e.errno != errno.ENOENT:\n",
    "                    raise\n",
    "                # create directory if not exists           \n",
    "                os.makedirs(destination_path)\n",
    "                # copy SQL files to new directory          \n",
    "                shutil.copyfile(filename,destination_full_path)\n",
    "        #######end copying files to new directory##############\n",
    "\n",
    "\n",
    "        #######reading file and replacing model syntax differences########\n",
    "            # Read in the file\n",
    "            with open(destination_full_path, 'r') as file :\n",
    "                filedata = file.read()\n",
    "                #getting header data to be replaced\n",
    "                match = re.search(r'{{[ ]{0,20}config[ ]{0,20}\\(', filedata)\n",
    "                if match:\n",
    "                    header=filedata[filedata.find(\"{{\"):filedata.find(\"}}\")+2]\n",
    "                    header_old=header\n",
    "                else:\n",
    "                    header=\"{{ config (\\n  materialized= 'view',\\n)\\n}}\"\n",
    "                    header_old=header\n",
    "                #dictionary to replace simple syntax elements\n",
    "                header_replace_dict = {\"{{\":\"\", \"}}\":\"}\", \"materialized\":\"type\",\"=\":\":\",\"(\":\"{\",\")\":\"\",\"'\":'\"',\"unique_key\":'uniqueKey'}\n",
    "\n",
    "                #iterate through dictionary keys\n",
    "                for key in header_replace_dict.keys():\n",
    "                    #replacing all headers based on dict mapping to 'header' variable\n",
    "                    header = header.replace(key, header_replace_dict[key])         \n",
    "\n",
    "\n",
    "            #writing file with replaced header and models references\n",
    "            with open(destination_full_path, 'w') as file:\n",
    "                #writting on file the replaced header based on dictionary \n",
    "                new_file = filedata\n",
    "                if not match:\n",
    "                    new_file = header_old+new_file\n",
    "\n",
    "                new_file = new_file.replace(header_old,header)\n",
    "                \n",
    "                new_file = new_file.replace(header_old,header)\n",
    "                #converting transient tables to snowflake specific block\n",
    "                \n",
    "                new_file = re.sub(r'(transient)[ ]{0,9}[\\:][ ]{0,9}(true|false|True|False|TRUE|FALSE)[,]{0,1}',r'snowflake: { \\n     \\1: \\2 \\n  }, \\n',new_file)\n",
    "                #converting disabled models syntax\n",
    "                new_file = re.sub(r'(enabled)[ ]{0,9}[\\:][ ]{0,9}(true|True|TRUE)','',new_file)\n",
    "                new_file = re.sub(r'(enabled)[ ]{0,9}[\\:][ ]{0,9}(false|False|FALSE)','disabled: true',new_file)\n",
    "                new_file = re.sub(r'(enabled)[ ]{0,9}[\\:][ ]{0,9}(var)[\\{][\\\"](.*)(\\\")',r'disabled: !constants.\\3',new_file)\n",
    "                #converting schema variables\n",
    "                new_file = re.sub(r'(schema)[ ]{0,9}[\\:][ ]{0,9}(var)[\\{][\\\"](.*)(\\\")',r'schema: constants.\\3',new_file)\n",
    "                #removing configurations that does not exists on dataform\n",
    "                new_file = re.sub(r'(pre_hook|post_hook|alias|meta|persist_docs|merge_update_columns|on_schema_change)[ ]{0,9}[\\:][ ]{0,9}.*[,}\\\")]','',new_file)\n",
    "                #changing {{ref to ${ref\n",
    "                new_file = re.sub(r\"[{][{][ ]{0,6}(ref|REF|Ref|SOURCE|source|Source)[ ]{0,9}[(][ ]{0,9}[']?\",'${ref(\"', new_file)     \n",
    "                #including just 1 space before config header\n",
    "                new_file = re.sub(r'(config)[ ]{0,40}[{]{0,40}','config {',new_file)\n",
    "                #closing ) with \") after sources and refs\n",
    "                new_file = re.sub(r\"(['][)][ ]{0,9}[}])\",'\")',new_file)\n",
    "                #removing alone commmas on start of config blockan\n",
    "                new_file = re.sub(r\"({)\\n[ ]{0,100}[,]\",'{',new_file)\n",
    "                #changing syntax reference to macros from {{ to ${\n",
    "                new_file = re.sub(r\"(?<!\\')({{)[ ]{0,20}[.]{0,20}\",'${common.',new_file)\n",
    "                new_file = re.sub(r\"([\\$].+)[ ]{0,9}(?<=\\()(')\",r'\\1\"',new_file)\n",
    "                #changing ' to \" on sources references\n",
    "                new_file = re.sub(r\"([\\$].+)((?<=[0-9A-Za-z])[ ]{0,9}[_'][.]{0,10}[ ]{0,20}[,][.]{0,20}[ ]{0,20}[.]{0,20}['])\",r'\\1\",\"',new_file)\n",
    "                #changing invocation_id to snowflake uuid_string\n",
    "                new_file = re.sub(r\"('{{invocation_id}}'|{{invocation_id}})\",'(SELECT invocation_id FROM ${ref(\"H_INVOCATION_ID\")})',new_file)\n",
    "                #changing incremental loads macro\n",
    "                new_file = re.sub(r\"({[ ]{0,10}%[ ]{0,10}if[ ]{0,10}is_incremental.*)((\\s*)(.*)\\s*)({%[ ]{0,10}endif..*})\",r'${ when(incremental(), \\n`\\4`) }',new_file)\n",
    "                new_file = re.sub(r\"((?:\\${[ ]{0,10}when.*(\\s.*)))(\\${this\\}\\})\",r'\\1${self()}',new_file)\n",
    "                #variables replacing                \n",
    "                #new_file = re.sub(r\"({[ ]{0,10}%[ ]{0,10}if[ ]{0,10}var[ ]{0,9}(\\(')(.*)(\\'\\)).*)\\n((.+\\n)+)([ ]{0,10}{%[ ]{0,10}endif..*})\",r'${ when(constants.\\3, \\n`\\5`) }',new_file)\n",
    "                new_file = re.sub(r\"({[ ]{0,10}%[ ]{0,10}if[ ]{0,10}var[ ]{0,9}(\\(')(.*)(\\'\\)).*)\",r'${ when(constants.\\3,\\n`',new_file)\n",
    "                new_file = re.sub(r\"([ ]{0,10}{%[ ]{0,10}endif..*})\",r'`)}',new_file)\n",
    "                new_file = re.sub(r\"\\${ref[ ]{0,9}[\\(]\\\"var(\\(\\\"(.*?)'\\))[ ]{0,9}[,](['\\\"]{0,9}(.*)(\\\"))\",r'${ref(constants.\\2,\"\\4\"',new_file)\n",
    "                new_file = \"\\npre_operations {\\n alter session set query_tag = 'dataform|${dataform.projectConfig.defaultSchema}|${name()}'\\n}\"\n",
    "                try:\n",
    "                    file.write(new_file)                \n",
    "                    print('Generated file: '+destination_full_path)\n",
    "                except IOError as e:\n",
    "                    print (\"I/O error({0}): {1}\".format(e.errno, e.strerror))\n",
    "        #######end reading file and replacing model syntax differences########\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function used to convert DBT SQL model files to JS model files on dataform\n",
    "def create_js_model_files(dbt_models_file_path,dataform_output_includes_path):\n",
    "\n",
    "    #check if source directory exists        \n",
    "    isExistSource = os.path.exists(dbt_models_file_path)\n",
    "    if not isExistSource:\n",
    "        print('Directory does not exists:' + dbt_models_file_path)\n",
    "    else:\n",
    "        model_files_path = find_files(dbt_models_file_path, '*.sql')\n",
    "\n",
    "\n",
    "        with open(dataform_output_includes_path+'/H_INVOCATION_ID.js', 'w+') as f:\n",
    "            f.write('module.exports = (params) => {\\n  return publish(\"H_INVOCATION_ID\",\\n {  type: \"table\",\\n...params.defaultConfig\\n}).query(ctx => ` SELECT uuid_string() as invocation_id from dual \\n`)\\n}')\n",
    "\n",
    "        #iterate on each SQL file on DBT directory and getting variables\n",
    "        for filename in model_files_path:  \n",
    "            single_file_name = os.path.basename(filename)\n",
    "            destination_path = os.path.dirname(filename)\n",
    "            destination_path = dataform_output_includes_path+(destination_path.split('models')[1])+'/'\n",
    "            destination_full_path = (destination_path+(os.path.basename(filename)).replace(\"sql\",\"js\"))\n",
    "\n",
    "         #######copying files to new directory##############   \n",
    "            try:\n",
    "                #copy files if directory already exists            \n",
    "                shutil.copyfile(filename,destination_full_path)\n",
    "            except IOError as e:\n",
    "                # ENOENT(2): file does not exist, raised also on missing dest parent dir\n",
    "                if e.errno != errno.ENOENT:\n",
    "                    raise\n",
    "                # create directory if not exists           \n",
    "                os.makedirs(destination_path)\n",
    "                # copy SQL files to new directory          \n",
    "                shutil.copyfile(filename,destination_full_path)\n",
    "        #######end copying files to new directory##############\n",
    "\n",
    "\n",
    "        #######reading file and replacing model syntax differences########\n",
    "            # Read in the file\n",
    "            with open(destination_full_path, 'r') as file :\n",
    "                filedata = file.read()\n",
    "                #getting header data to be replaced\n",
    "                match = re.search(r'{{[ ]{0,20}config[ ]{0,20}\\(', filedata)\n",
    "                if match:\n",
    "                    header=filedata[filedata.find(\"{{\"):filedata.find(\"}}\")+2]\n",
    "                    header_old=header\n",
    "                else:\n",
    "                    header=\"{{ config (\\n  materialized= 'view',\\n)\\n}}\"\n",
    "                    header_old=header\n",
    "\n",
    "\n",
    "                #dictionary to replace simple syntax elements                \n",
    "                header_replace_dict1 = {\"{{\":\"\", \"materialized\":\"type\",\"=\":\":\",\"(\":\"{\",\")\":\"\",\"'\":'\"',\"unique_key\":'uniqueKey'}\n",
    "                header_replace_dict2 = {\"}}\":\"    ...params.defaultConfig\\n}).query(ctx => `\"}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                #iterate through dictionary keys\n",
    "                for key in header_replace_dict1.keys():\n",
    "                    #replacing all headers based on dict mapping to 'header' variable\n",
    "                    #header = header.replace(key, header_replace_dict2[key])         \n",
    "                    header = header.replace(key, header_replace_dict1[key])\n",
    "\n",
    "\n",
    "                       #iterate through dictionary keys\n",
    "                for key in header_replace_dict2.keys():\n",
    "                    #replacing all headers based on dict mapping to 'header' variable\n",
    "                    #header = header.replace(key, header_replace_dict2[key])         \n",
    "                    header = header.replace(key, header_replace_dict2[key])\n",
    "\n",
    "                #header=re.sub(r'\\\"]$',r'\"],',header)\n",
    "                #header=re.sub(r'\\\"$',r'\",',header)\n",
    "\n",
    "                header = re.sub(r'(\\\"]{1})$','\"],',header,flags=re.MULTILINE)\n",
    "                header = re.sub(r'(\\\"]{0})$','\",',header,flags=re.MULTILINE)\n",
    "\n",
    "\n",
    "\n",
    "            #writing file with replaced header and models references\n",
    "            with open(destination_full_path, 'w') as file:\n",
    "                #writting on file the replaced header based on dictionary \n",
    "\n",
    "                new_file = filedata\n",
    "                if not match:\n",
    "                    new_file = header_old+new_file\n",
    "\n",
    "                new_file = new_file.replace(header_old,header)\n",
    "\n",
    "                #converting transient tables to snowflake specific block\n",
    "                new_file = re.sub(r'(transient)[ ]{0,9}[\\:][ ]{0,9}(true|false|True|False|TRUE|FALSE)[,]{0,1}',r'snowflake: { \\n     \\1: \\2 \\n  }, \\n',new_file)\n",
    "                #converting disabled models syntax\n",
    "                new_file = re.sub(r'(enabled)[ ]{0,9}[\\:][ ]{0,9}(true|True|TRUE)','',new_file)\n",
    "                new_file = re.sub(r'(enabled)[ ]{0,9}[\\:][ ]{0,9}(false|False|FALSE)','disabled: true',new_file)\n",
    "                new_file = re.sub(r'(enabled)[ ]{0,9}[\\:][ ]{0,9}(var)[\\{][\\\"](.*)(\\\")',r'disabled: !params.\\3',new_file)\n",
    "                #converting schema variables\n",
    "\n",
    "                new_file = re.sub(r'(schema)[ ]{0,9}[\\:][ ]{0,9}(var)[\\{][\\\"](.*)(\\\")',r'schema: params.\\3',new_file)   \n",
    "                new_file = re.sub(r'((schema):[ ].*)(\\\"\\,)(.*)',r'\\1,',new_file)\n",
    "\n",
    "                #removing configurations that does not exists on dataform\n",
    "                new_file = re.sub(r'(pre_hook|post_hook|alias|meta|persist_docs|merge_update_columns|on_schema_change)[ ]{0,9}[\\:][ ]{0,9}.*[,}\\\")]','',new_file)\n",
    "                #changing {{ref to ${ref\n",
    "                new_file = re.sub(r\"[{][{][ ]{0,6}(ref|REF|Ref|SOURCE|source|Source)[ ]{0,9}[(][ ]{0,9}[']?\",'${ctx.ref(\"', new_file)     \n",
    "\n",
    "                new_file = re.sub(r\"(\\${ctx.ref)\\((\\\"var\\((\\'(.*)\\'))\\,.*\\),[ ]{0,9}(\\'(.*)\\').*\",r'\\1(params.\\4,\"\\6\")}', new_file)     \n",
    "\n",
    "                #including just 1 space before config header\n",
    "                new_file = re.sub(r'(config)[ ]{0,40}[{]{0,40}','module.exports = (params) => {\\n  return publish(\"{name_of_the_model}\", {',new_file)\n",
    "\n",
    "                #closing ) with \") after sources and refs\n",
    "                new_file = re.sub(r\"(['][)][ ]{0,9}[}])\",'\")',new_file)\n",
    "\n",
    "                #removing alone commmas on start of config blockan\n",
    "                new_file = re.sub(r\"({)\\n[ ]{0,100}[,]\",'{',new_file)\n",
    "                #changing syntax reference to macros from {{ to ${\n",
    "                new_file = re.sub(r\"(?<!\\')({{)[ ]{0,20}[.]{0,20}\",'${common.',new_file)\n",
    "                new_file = re.sub(r\"([\\$].+)[ ]{0,9}(?<=\\()(')\",r'\\1\"',new_file)\n",
    "                #changing ' to \" on sources references\n",
    "                new_file = re.sub(r\"([\\$].+)((?<=[0-9A-Za-z])[ ]{0,9}[_'][.]{0,10}[ ]{0,20}[,][.]{0,20}[ ]{0,20}[.]{0,20}['])\",r'\\1\",\"',new_file)\n",
    "                #changing invocation_id to snowflake uuid_string\n",
    "                new_file = re.sub(r\"('{{invocation_id}}'|{{invocation_id}})\",'(SELECT invocation_id FROM ${ctx.ref(\"H_INVOCATION_ID\")})',new_file)\n",
    "                #changing incremental loads macro\n",
    "                new_file = re.sub(r\"({[ ]{0,10}%[ ]{0,10}if[ ]{0,10}is_incremental.*)((\\s*)(.*)\\s*)({%[ ]{0,10}endif..*})\",r'${ ctx.when(incremental(), \\n`\\4`) }',new_file)\n",
    "                new_file = re.sub(r\"((?:\\${[ ]{0,10}when.*(\\s.*)))(\\${this\\}\\})\",r'\\1${self()}',new_file)\n",
    "                #variables replacing\n",
    "                #new_file = re.sub(r\"({[ ]{0,10}%[ ]{0,10}if[ ]{0,10}var[ ]{0,9}(\\(')(.*)(\\'\\)).*)\\n((.+\\n)+)([ ]{0,10}{%[ ]{0,10}endif..*})\",r'${ ctx.when(params.\\3, \\n`\\5`) }',new_file)\n",
    "                new_file = re.sub(r\"({[ ]{0,10}%[ ]{0,10}if[ ]{0,10}var[ ]{0,9}(\\(')(.*)(\\'\\)).*)\",r'${ ctx.when(params.\\3,\\n`',new_file)\n",
    "                new_file = re.sub(r\"([ ]{0,10}{%[ ]{0,10}endif..*})\",r'`)}',new_file)\n",
    "\n",
    "                new_file = new_file.replace('{name_of_the_model}',single_file_name.replace('.sql',''))\n",
    "                new_file = re.sub(r\"\\${ctx.ref[ ]{0,9}[\\(]\\\"var(\\(\\\"(.*?)'\\))[ ]{0,9}[,](['\\\"]{0,9}(.*)(\\\"))\",r\"${ctx.ref(params.\\2,'\\4'\",new_file)\n",
    "                #end of file\n",
    "                new_file = re.sub(r\"\\Z\",r\"\\n`).preOps(ctx => `\\n alter session set query_tag = 'dataform|${dataform.projectConfig.defaultSchema}|${ctx.name()}'`\\n )\\n}\",new_file)\n",
    "\n",
    "                try:\n",
    "                    file.write(new_file)                \n",
    "                    print('Generated file: '+destination_full_path)\n",
    "                except IOError as e:\n",
    "                    print (\"I/O error({0}): {1}\".format(e.errno, e.strerror))\n",
    "        #######end reading file and replacing model syntax differences########\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sqlx_snapshot_files(dbt_snapshots_file_path,dataform_output_models_path):\n",
    "     \n",
    "    #check if source directory exists        \n",
    "    isExistSource = os.path.exists(dbt_snapshots_file_path)\n",
    "    if not isExistSource:\n",
    "        print('Directory does not exists:' + dbt_snapshots_file_path)\n",
    "    else:\n",
    "        snapshots_file_path = find_files(dbt_snapshots_file_path, '*.sql')\n",
    "        \n",
    "        for filename in snapshots_file_path:  \n",
    "            single_file_name = os.path.basename(filename)\n",
    "            destination_path = os.path.dirname(filename)\n",
    "            destination_path = dataform_output_models_path+'/snapshots'+(destination_path.split('snapshots')[1])+'/'\n",
    "            destination_full_path = (destination_path+(os.path.basename(filename)).replace(\".sql\",\".js\"))\n",
    "\n",
    "         #######copying files to new directory##############   \n",
    "            try:\n",
    "                #copy files if directory already exists\n",
    "\n",
    "                shutil.copyfile(filename,destination_full_path)\n",
    "            except IOError as e:\n",
    "                # ENOENT(2): file does not exist, raised also on missing dest parent dir\n",
    "                if e.errno != errno.ENOENT:\n",
    "                    raise\n",
    "                # create directory if not exists\n",
    "                os.makedirs(destination_path)\n",
    "                # copy source files to new directory\n",
    "                shutil.copyfile(filename,destination_full_path)\n",
    "        #######end copying files to new directory##############\n",
    "\n",
    "            #read copied files\n",
    "            with open(destination_full_path, 'r') as file :\n",
    "                filedata = file.read()   \n",
    "                #getting the name of the snapshot source table\n",
    "                table = re.search(r\"({{)[ ]{0,9}(ref).*((?<=').*(?='))\",filedata).group(3)\n",
    "                #getting the name of the file and removing extension\n",
    "                file_name = single_file_name.replace('.sql','')\n",
    "                #removing macro references \n",
    "                filedata = re.sub(r'({%)[ ]{0,10}(endsnapshot|snapshot).*(})','',filedata)\n",
    "                #if exists check cols (not supported to data form), replace for timestamp field\n",
    "                filedata = re.sub(r\"(check_cols.*[\\]]{1,1}|check_cols.*[\\\"]{1,1})\",'timestamp: '+dlh_timestamp_field,filedata)\n",
    "                #getting header block\n",
    "                filedata=filedata[filedata.find(\"{\"):filedata.find(\"}\")+2]\n",
    "                #replacing fields on header block\n",
    "                filedata = filedata.replace(')', 'source: {\\n schema: \"{{schema}}\",\\n name: \"{{table}}\",\\n}, \\n});')\n",
    "                filedata = filedata.replace('{{schema}}',target_schema).replace('{{table}}',table)\n",
    "                #create a dict with some syntax differences\n",
    "                replace_dict = {\"updated_at\":\"timestamp\",\"unique_key\":\"  uniqueKey\", \"config\":'scd(\"{{source_data_scd}}\", ',\"'\":'\"',\"{{\":\"\",\"}}\":\"\",\"=\":\": \"}\n",
    "\n",
    "                #iterate through dictionary keys\n",
    "                for key in replace_dict.keys():\n",
    "                    #replacing all headers\n",
    "                    filedata = filedata.replace(key, replace_dict[key]).replace('{{source_data_scd}}',file_name)\n",
    "                    filedata = re.sub(r'(scd\\(.*)(\\()',r'\\1{',filedata)\n",
    "\n",
    "\n",
    "            #writing file with replaced header and models references\n",
    "            with open(destination_full_path, 'w') as file:\n",
    "                #replacing files content\n",
    "                new_file = filedata\n",
    "                new_file = re.sub(r'(strategy)[ ]{0,9}[\\:][ ]{0,9}.*(,)','',new_file)   \n",
    "                #removing unsupported configurations\n",
    "                new_file = re.sub(r'(target_database|target_schema|strategy|invalidate_hard_deletes|check_cols)[ ]{0,9}[\\:][ ]{0,9}.*[,}\\\"\\')]','',new_file)\n",
    "                new_file = re.sub(r\"[ ]{2,999}\",'',new_file)\n",
    "                new_file = re.sub(r\"^\\s*$\",'',new_file,re.MULTILINE)\n",
    "                new_file = re.sub(r'\\n\\s*\\n','\\n',new_file,re.MULTILINE)            \n",
    "                #insert space to ident\n",
    "                new_file = new_file.replace('uniqueKey:','  uniqueKey:').replace('timestamp:','  timestamp:').replace('source:','  source:').replace('schema:','   schema:').replace('name:','   name:').replace('},','  },')\n",
    "                #create .JS snapshot file\n",
    "                try:\n",
    "                    file.write('const scd = require(\"dataform-scd\");\\n\\n')\n",
    "                    file.write(new_file)\n",
    "                    print('Generated file: '+destination_full_path)\n",
    "                except IOError as e:\n",
    "                    print (\"I/O error({0}): {1}\".format(e.errno, e.strerror))\n",
    "\n",
    "        #######end reading file and replacing model syntax differences########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataform_install_configuration(dataform_root_path,dataform_credentials_file_path,current_directory):\n",
    "    \n",
    "    try:\n",
    "        shutil.rmtree(dataform_root_path)\n",
    "    except OSError as err:\n",
    "        print(err)\n",
    "\n",
    "        \n",
    "    #initiating dataform new project\n",
    "    os.system(\"dataform init snowflake \"+dataform_root_path)\n",
    "    #copying snowflake credentials file\n",
    "    shutil.copy(dataform_credentials_file_path,dataform_root_path)\n",
    "    \n",
    "    packages_file = dataform_root_path+ '/' +'package.json'\n",
    "    dataform_json_file = dataform_root_path+ '/' +'dataform.json'\n",
    "    \n",
    "    with open(dataform_json_file, 'r') as dataform_file:\n",
    "        dataform_json_file_data = dataform_file.read()\n",
    "        filedata= re.sub(r'(\\\"defaultSchema\\\"[:]{0,9}[ ]{0,9})(.*)\\,',r'\\1 \"'+target_schema+'\",', dataform_json_file_data)\n",
    "        filedata= re.sub(r'(\\\"assertionSchema\\\"[:]{0,9}[ ]{0,9})(.*)\\,',r'\\1 \"'+target_schema+'\",', filedata)\n",
    "        \n",
    "    with open(dataform_json_file, \"w\") as dataform_file:        \n",
    "        new_file = filedata\n",
    "        dataform_file.write(new_file)\n",
    "        \n",
    "        \n",
    "    with open(packages_file, 'r') as file:\n",
    "        packages_file_data = file.read()\n",
    "    \n",
    "    #setting version 19 of dataform\n",
    "    with open(packages_file, \"w\") as file:\n",
    "        #replacing package.json file to version 19 and including scd package\n",
    "        filedata = re.sub(r'(\\\"@dataform).*',r'\"@dataform/core\": \"1.19.0\",\\n        \"dataform-scd\": \"git+https://github.com/dataform-co/dataform-scd.git#0.1\"', packages_file_data)\n",
    "        file.write(filedata)\n",
    "    \n",
    "    #changing path to dataform roots    \n",
    "    os.chdir(dataform_root_path)\n",
    "       \n",
    "    #installing version 0.19 and dataform scd\n",
    "    os.system(\"dataform install\")\n",
    "    \n",
    "    #getting back to default directory\n",
    "    os.chdir(current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataform_assertions_documentation(dbt_models_file_path,dataform_output_models_path,dataform_output_includes_path,conversion_type):\n",
    "\n",
    "     #check if source directory exists        \n",
    "    isExistSource = os.path.exists(dbt_models_file_path)\n",
    "    if not isExistSource:\n",
    "        print('Directory does not exists:' + dbt_models_file_path)\n",
    "    else:\n",
    "        schema_files_path = find_files(dbt_models_file_path, '*.yml')\n",
    "\n",
    "        for file in schema_files_path:\n",
    "            #reading all YAML files on source directory\n",
    "            dic = read_yaml_file(file)\n",
    "            #getting only sources entity from yaml file\n",
    "            if 'models' in dic.keys():\n",
    "\n",
    "                models_dir = dic[\"models\"]\n",
    "                dictionary={}\n",
    "                #print(models_dir)\n",
    "                unique_list=[]\n",
    "                not_null_list=[]\n",
    "                not_null = {} \n",
    "                table_descriptions = {} \n",
    "                description={}\n",
    "                unique = {}  \n",
    "                tables_list=[]\n",
    "                description_list=[]\n",
    "                table_description_list=[]\n",
    "                jmodel_tables_description=''\n",
    "                jmodel_description=''\n",
    "                jmodel=''\n",
    "                for model in models_dir:\n",
    "                    #declaring variables for each element of dictionary that will be used. tables = list, schema and database = str        \n",
    "                    #print(model[\"name\"])\n",
    "                    if 'columns' in model:\n",
    "                        columns = (model[\"columns\"]) \n",
    "                    if 'name' in model:\n",
    "                        tables = (model[\"name\"])\n",
    "\n",
    "\n",
    "                    tables_list.append(tables)\n",
    "                    #getting tables descriptions\n",
    "                    if 'description' in model: \n",
    "                        descriptions = (model[\"description\"])\n",
    "                        table_descriptions = {descriptions:tables}\n",
    "                        table_description_list.append(table_descriptions.copy())\n",
    "\n",
    "\n",
    "                    #getting column tests and descriptions\n",
    "                    for column in columns:\n",
    "                        column_get = column.get('name')\n",
    "\n",
    "\n",
    "                        if 'tests' in column:\n",
    "                            tests= (column.get('tests'))\n",
    "\n",
    "\n",
    "\n",
    "                            if 'unique' in tests:\n",
    "                                unique = {column_get:tables}                                                        \n",
    "                                unique_list.append(unique.copy())\n",
    "\n",
    "\n",
    "                            if 'not_null' in tests:\n",
    "                                not_null = {column_get:tables}\n",
    "                                not_null_list.append(not_null.copy())\n",
    "\n",
    "\n",
    "\n",
    "                        if 'description'in column:\n",
    "                            descriptions= (column.get('description'))\n",
    "                            #description = {'|    '+column_get+': '+\"'\"+descriptions+\"'\":tables}\n",
    "                            description = {'|    '+column_get+': '+'\"'+descriptions+'\",': tables}\n",
    "                            description_list.append(description.copy())\n",
    "\n",
    "\n",
    "                #print(not_null_list)\n",
    "\n",
    "                keyfunc = lambda d: next(iter(d.values()))   \n",
    "\n",
    "                #print('Found table description' +str(table_description_list)+'\\n')\n",
    "                #print('Found uniqueKey test' +str(unique_list)+'\\n')\n",
    "                #print('Found Not_Null test' +str(not_null_list)+'\\n')\n",
    "                #print('Found column description' +str(description_list)+'\\n')\n",
    "\n",
    "                #create dictionary for data tests and tables/columns descriptions\n",
    "                not_null_dict={k: [x for d in g for x in d] \n",
    "                    for k, g in itertools.groupby(sorted(not_null_list, key=keyfunc), key=keyfunc)}\n",
    "\n",
    "                unique_dict={k: [x for d in g for x in d] \n",
    "                    for k, g in itertools.groupby(sorted(unique_list, key=keyfunc), key=keyfunc)}\n",
    "\n",
    "                description_dict={k: [x for d in g for x in d] \n",
    "                    for k, g in itertools.groupby(sorted(description_list, key=keyfunc), key=keyfunc)}\n",
    "\n",
    "                table_description_dict={k: [x for d in g for x in d] \n",
    "                        for k, g in itertools.groupby(sorted(table_description_list, key=keyfunc), key=keyfunc)}\n",
    "\n",
    "\n",
    "                # list of all .sql files in a directory\n",
    "                if conversion_type == 'sqlx':\n",
    "                    dataform_files = find_files(dataform_output_models_path, '*.sqlx')\n",
    "                else:\n",
    "                    dataform_files = find_files(dataform_output_includes_path, '*.js')\n",
    "\n",
    "                files_list =[]\n",
    "                file_name_list =[]\n",
    "\n",
    "                #getting all model files on dataform path\n",
    "                for file in dataform_files:\n",
    "                    #only tables that has descriptions or tests\n",
    "                    if os.path.basename(file).replace('.sqlx','').replace('.js','') in tables_list:\n",
    "                        file_name = os.path.basename(file)\n",
    "                        destination_path = os.path.dirname(file)\n",
    "                        full_path = destination_path+'/'+file_name     \n",
    "                        files_list.append(file_name.replace('.sqlx','').replace('.js',''))\n",
    "\n",
    "\n",
    "                        #read files\n",
    "                        with open(full_path, 'r') as file :\n",
    "\n",
    "                            file_name = file_name.replace('.sqlx','').replace('.js','')\n",
    "                            filedata = file.read()\n",
    "\n",
    "\n",
    "                            #add description to table (setting variable)\n",
    "                            if file_name in table_description_dict.keys():\n",
    "                                jmodel_tables_description='  description: '+str(table_description_dict[file_name]).replace(\"'\",'\"').replace('[','').replace(']','')+',\\n'\n",
    "\n",
    "                            #add assertions { on variable\n",
    "\n",
    "                            if file_name in unique_dict.keys() or file_name in unique_dict.keys():\n",
    "                                jmodel='  assertions: {\\n'\n",
    "\n",
    "\n",
    "                                #when exists uniqueKey test on dbt, create on dataform\n",
    "                                if file_name in unique_dict.keys():                    \n",
    "                                    jmodel+='    uniqueKey: '+str(unique_dict[file_name]).replace(\"'\",'\"')+',\\n'\n",
    "\n",
    "                                #when exists NotNull test on dbt, create on dataform\n",
    "                                if file_name in not_null_dict.keys():\n",
    "                                    jmodel+='    nonNull: '+str(not_null_dict[file_name]).replace(\"'\",'\"')+'\\n'\n",
    "\n",
    "\n",
    "                                jmodel+='\\n  },'\n",
    "\n",
    "                            #add column description on dataform\n",
    "                            if file_name in description_dict.keys():         \n",
    "                                jmodel_description = '  columns: {\\n'\n",
    "                                jmodel_description += str(description_dict[file_name]).replace(\"\\\\n\",'').replace(\"\\\\\",'').replace('[','').replace(\"']\",'').replace(\"'|\",' \\n').replace(\"',\",'')\n",
    "                                jmodel_description += '\\n  },\\n'\n",
    "\n",
    "                            if conversion_type == 'sqlx':\n",
    "                                #setting variable with all tests and description to be added on model\n",
    "                                header=filedata[filedata.find(\"config {\"):filedata.find(\"}\")+2]                \n",
    "                                header_old=header\n",
    "                                header = header+jmodel_tables_description+jmodel_description+jmodel\n",
    "\n",
    "                                with open(f'{full_path}', \"w\") as jmodel_file:\n",
    "                                    #replace old header to new header with assertions and column or table documentations\n",
    "                                    match = re.search(r'config[ ]{0,20}\\{', filedata)\n",
    "                                    if match:\n",
    "                                        jmodel_file.write(filedata.replace(header_old,header))\n",
    "                                    else:\n",
    "                                        jmodel_file.write('config {\\n'+header+'\\n}\\n'+filedata)\n",
    "\n",
    "                            else:\n",
    "                                  #setting variable with all tests and description to be added on model\n",
    "                                header=filedata[filedata.find(\"module.exports = (params) => {\"):filedata.find(\"...params.defaultConfig\")]                \n",
    "                                header_old=header\n",
    "                                header = header+jmodel_tables_description+jmodel_description+jmodel+'\\n'\n",
    "                            \n",
    "                                \n",
    "                                with open(f'{full_path}', \"w\") as jmodel_file:\n",
    "                                        #replace old header to new header with assertions and column or table documentations\n",
    "                                        #match = re.search(r'config[ ]{0,20}\\{', filedata)\n",
    "                                        #if match:\n",
    "                                    jmodel_file.write(filedata.replace(header_old,header))\n",
    "                                        #else:\n",
    "                                         #   jmodel_file.write('config {\\n'+header+'\\n}\\n'+filed\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function used to convert source (YML) to (SQLX) file\n",
    "def create_project_variables(dbt_source_project_path,dataform_root_path):\n",
    "\n",
    "\n",
    "\n",
    "    #check if source directory exists        \n",
    "    isExistSource = os.path.exists(dbt_source_project_path)\n",
    "    if not isExistSource:\n",
    "        print('Directory does not exists:' + dbt_source_project_path)\n",
    "    else:\n",
    "        variables_file = find_files(dbt_source_project_path, 'dbt_project.yml')\n",
    "        dataform_file = dataform_root_path+'/includes/constants.js'\n",
    "\n",
    "\n",
    "    #iterate through each yaml file that contains sources\n",
    "        for file in variables_file:\n",
    "            #reading all YAML files on source directory\n",
    "            dic = read_yaml_file(file)\n",
    "\n",
    "            #getting only sources entity from yaml file\n",
    "            jmodel=''\n",
    "            if \"vars\" in dic.keys():\n",
    "\n",
    "                variables = dic[\"vars\"]            \n",
    "                jmodel = ''\n",
    "                if conversion_type == 'sqlx':\n",
    "\n",
    "                    for key, value in variables.items():\n",
    "                        print(str(key))\n",
    "                        jmodel+='const '+str(key)+' = '+'\"'+str(value).replace(\"'\",'\"')+'\";'+'\\n'\n",
    "\n",
    "\n",
    "                    jmodel+='module.exports = {\\n'\n",
    "                    for key, value in variables.items():\n",
    "                        jmodel+='  '+str(key)+',\\n'\n",
    "\n",
    "                    #replacing list_fields\n",
    "                    jmodel = re.sub(r'(= \\\"\\[)(.*)(\\]\\\")',r'= [\\2]',jmodel)\n",
    "                    #replacing date_fields\n",
    "                    jmodel = re.sub(r\"[\\\"]([0-9]{4}[-][0-9]{2}[-][0-9]{2})[\\\"]\",'\"'+r\"'\\1'\"+'\"',jmodel)\n",
    "                    #replacing booleans\n",
    "                    jmodel = re.sub(r'[\\=][ ]{0,9}(\"TRUE\"|\"true\"|\"True\")',r\"= true\",jmodel)\n",
    "                    jmodel = re.sub(r'[\\=][ ]{0,9}(\"FALSE\"|\"false\"|\"False\")',r\"= false\",jmodel)\n",
    "                    jmodel = jmodel[:-2]\n",
    "                    jmodel+='\\n}'\n",
    "\n",
    "\n",
    "                    with open(f'{dataform_file}', \"w\") as jmodel_file:\n",
    "                    #replace old header to new header with assertions and column or table documentations\n",
    "                        jmodel_file.write(jmodel)\n",
    "                        jmodel_file.close\n",
    "                       #print(jmodel)\n",
    "\n",
    "\n",
    "                else:\n",
    "                    jmodel += '  params = {\\n'\n",
    "                    const = ''\n",
    "                    for key, value in variables.items():\n",
    "                        jmodel+='    '+str(key)+': '+\"'\"+str(value)+\"',\"+'\\n'\n",
    "                        const += '    '+str(key)+',\\n'\n",
    "                    jmodel+='    ...params\\n  };\\n'\n",
    "                    jmodel+='  const {\\n'\n",
    "                    jmodel+=const\n",
    "\n",
    "\n",
    "                    #replacing list_fields\n",
    "                    jmodel = re.sub(r'(= \\\"\\[)(.*)(\\]\\\")',r'= [\\2]',jmodel)                \n",
    "                    #replacing booleans\n",
    "                    jmodel = re.sub(r\"[\\:][ ]{0,9}('TRUE'|'true'|'True')\",r\": true\",jmodel)\n",
    "                    jmodel = re.sub(r\"[\\:][ ]{0,9}('FALSE'|'false'|'False')\",r\": false\",jmodel)\n",
    "                    jmodel = jmodel[:-2]\n",
    "                    jmodel+='\\n} = params;'\n",
    "\n",
    "\n",
    "                    params_sources = '\\n'\n",
    "                    return (jmodel+'\\n'+params_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_file(dataform_root_path):\n",
    "\n",
    "\n",
    "    #check if source directory exists        \n",
    "    isExistSource = os.path.exists(dataform_root_path)\n",
    "    if not isExistSource:\n",
    "        print('Directory does not exists:' + dataform_root_path)\n",
    "\n",
    "    else:\n",
    "        js_files = find_files(dataform_root_path+'/includes/', '*.js')\n",
    "        dataform_file = dataform_root_path+'/index.js'\n",
    "        example_file = dataform_root_path+'/definitions/example.js'\n",
    "\n",
    "         #iterate through each yaml file that contains sources\n",
    "        jmodel = ''\n",
    "        jmodel += '\\n\\nmodule.exports = (params) => {\\n'\n",
    "        #jmodel += 'params = {\\n  ...params\\n}\\n'\n",
    "        #jmodel += 'const {\\n} = params;'\n",
    "        models = ''\n",
    "        #footer = '\\n\\nreturn {\\n'\n",
    "        footer=''\n",
    "        sources=''\n",
    "        example = f'const {target_schema} = require(\"../\");'\n",
    "        example += '\\n'\n",
    "        example += f'const models = {target_schema}('\n",
    "        example += '{\\n});'\n",
    "        for filename in js_files:\n",
    "            single_file_name = os.path.basename(filename)\n",
    "            file_name=single_file_name.replace('.js','')\n",
    "            destination_path = os.path.dirname(filename)\n",
    "            destination_path = dataform_root_path+'/includes'+(destination_path.split('includes')[1])+'/'\n",
    "            destination_full_path = (destination_path+(os.path.basename(filename)))\n",
    "            destination_full_path = './includes'+destination_full_path.split(\"/includes\",1)[1] \n",
    "            if single_file_name != 'sources.js' and single_file_name != 'constants.js':\n",
    "                models += 'const '+file_name+' = require(\"'+destination_full_path+'\");\\n'\n",
    "                footer += f'    {file_name}: {file_name}(params)'+',\\n'\n",
    "        models = str(models+jmodel)\n",
    "\n",
    "\n",
    "\n",
    "        variables = str(create_project_variables(dbt_source_project_path,dataform_root_path))\n",
    "\n",
    "        if variables == 'None':\n",
    "            variables = ''\n",
    "        else:\n",
    "            variables\n",
    "\n",
    "        sources = str(create_js_source_file(dbt_models_file_path,dataform_output_sources_path,conversion_type))\n",
    "\n",
    "        index_file = models+variables+sources+footer+'\\n  }\\n}'\n",
    "\n",
    "\n",
    "\n",
    "        with open(f'{dataform_file}', \"w\") as jmodel_file:\n",
    "            jmodel_file.write(index_file)\n",
    "            print(index_file)\n",
    "            print ('Created index.js file')\n",
    "\n",
    "        with open(f'{example_file}', \"w\") as jmodel_file:\n",
    "            jmodel_file.write(example)\n",
    "            print ('Created example.js file')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function used to convert source (YML) to (JS) file\n",
    "def create_macro_files(dbt_macros_file_path,dbt_models_file_path,dataform_output_includes_path,conversion_type):\n",
    "\n",
    "    #check if source directory exists        \n",
    "    isExistMacros = os.path.exists(dbt_macros_file_path)\n",
    "    if not isExistMacros:\n",
    "        print('Directory does not exists:' + dbt_macros_file_path)\n",
    "    else:\n",
    "        macro_files = find_files(dbt_macros_file_path, '*.sql')\n",
    "\n",
    "\n",
    "    #creating directory        \n",
    "        footer_macro_names=''\n",
    "        filecontent= ''\n",
    "        destination_full_path = dataform_output_includes_path+'/common.js'\n",
    "        #iterate on each SQL file on DBT directory and getting variables\n",
    "        for filename in macro_files:  \n",
    "            single_file_name = os.path.basename(filename)\n",
    "\n",
    "            \n",
    "            with open(filename, 'r') as file :\n",
    "                filedata = file.read()                    \n",
    "                macro_names = re.compile(r'({%[ ]{0,9}macro)(([! ])(.*)([ ]{0,10}\\())')\n",
    "\n",
    "                # find all macros\n",
    "                for match in macro_names.finditer(filedata):\n",
    "                    footer_macro_names += '    '+match.group(4)+',\\n'\n",
    "\n",
    "                filecontent = re.sub(r'({%[ ]{0,9}macro)','function ',filedata)\n",
    "                filecontent = re.sub(r'{[ ]{0,9}%[ ]{0,9}endmacro[ ]{0,9}%[ ]{0,9}}','`;\\n}',filecontent)\n",
    "                filecontent = re.sub(r'%}','{\\n   return `',filecontent)\n",
    "                filecontent = re.sub(r'`[ ]{0,20}\\n+','`',filecontent)\n",
    "                filecontent = re.sub(r'({{)([^ ]+)(}})',r'${\\2}',filecontent)\n",
    "                filecontent = re.sub(r'--','//',filecontent)\n",
    "\n",
    "\n",
    "        footer = '  module.exports = {\\n'+footer_macro_names[:-2]+'\\n  };'\n",
    "        new_macro_file = filecontent+footer       \n",
    "\n",
    "\n",
    "        with open(destination_full_path, 'a') as file:     \n",
    "\n",
    "            file.write(new_macro_file)\n",
    "\n",
    "        if conversion_type == 'js':\n",
    "            model_files = find_files(dataform_output_includes_path, '*.js')\n",
    "\n",
    "            #if the file has macros, add header \n",
    "            for filename in model_files:                \n",
    "                with open(filename, 'r') as file:\n",
    "\n",
    "                    filedata = file.read()\n",
    "                    match = re.search(r'(\\${[ ]{0,9}common.)', filedata)\n",
    "\n",
    "                    if match:\n",
    "                        header=f'const common = require(\"../../common\");\\n'\n",
    "                        with open(filename, 'w') as file:\n",
    "                            file.write(header+filedata)\n",
    "                            file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbt_dataform_converter(dataform_root_path,dbt_models_file_path,dbt_snapshots_file_path,dataform_credentials_file_path,dataform_output_models_path,dataform_output_includes_path,current_directory,conversion_type):\n",
    "    \n",
    "     #check if source directory exists        \n",
    "    isExistSource = os.path.exists(dbt_models_file_path)\n",
    "    if not isExistSource:\n",
    "        print('Source directory does not exists:' + dbt_models_file_path)\n",
    "    else:\n",
    "        #installing dataform\n",
    "        dataform_install_configuration(dataform_root_path,dataform_credentials_file_path,current_directory)    \n",
    "        #creating source       \n",
    "      \n",
    "        if conversion_type == 'js':            \n",
    "            create_js_model_files(dbt_models_file_path,dataform_output_includes_path)   \n",
    "            create_index_file(dataform_root_path)\n",
    "        else:            \n",
    "            create_js_source_file(dbt_models_file_path,dataform_output_sources_path,conversion_type)\n",
    "            create_sqlx_models_files(dbt_models_file_path,dataform_output_models_path)\n",
    "            #converting variables\n",
    "            create_project_variables(dbt_source_project_path,dataform_root_path)        \n",
    "        #creating snapshot files\n",
    "        create_sqlx_snapshot_files(dbt_snapshots_file_path,dataform_output_models_path)\n",
    "        #addind test to models\n",
    "        dataform_assertions_documentation(dbt_models_file_path,dataform_output_models_path,dataform_output_includes_path,conversion_type)\n",
    "        \n",
    "        create_macro_files(dbt_macros_file_path,dataform_output_includes_path,dataform_output_includes_path,conversion_type)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing project files...\n",
      "\n",
      "\u001b[32mDirectories successfully created:\u001b[0m\n",
      "  /Users/guilhermealcantara/OneDrive/brf consulting/dataform/test_bill_dot_com\n",
      "  /Users/guilhermealcantara/OneDrive/brf consulting/dataform/test_bill_dot_com/definitions\n",
      "  /Users/guilhermealcantara/OneDrive/brf consulting/dataform/test_bill_dot_com/includes\n",
      "\u001b[32mFiles successfully written:\u001b[0m\n",
      "  /Users/guilhermealcantara/OneDrive/brf consulting/dataform/test_bill_dot_com/dataform.json\n",
      "  /Users/guilhermealcantara/OneDrive/brf consulting/dataform/test_bill_dot_com/package.json\n",
      "  /Users/guilhermealcantara/OneDrive/brf consulting/dataform/test_bill_dot_com/.gitignore\n",
      "\u001b[32mNPM packages successfully installed.\u001b[0m\n",
      "Installing NPM dependencies...\n",
      "\n",
      "\u001b[32mProject dependencies successfully installed.\u001b[0m\n",
      "Generated file: ../../dataform/test_bill_dot_com/includes/staging/CUSTOMERS/V_BDC_CUSTOMERS_STG.js\n",
      "Generated file: ../../dataform/test_bill_dot_com/includes/staging/CUSTOMERS/V_BDC_CUSTOMERS_BANK_ACCOUNT_STG.js\n",
      "Generated file: ../../dataform/test_bill_dot_com/includes/staging/VENDORS/V_BDC_VENDORS_STG.js\n",
      "Generated file: ../../dataform/test_bill_dot_com/includes/staging/VENDORS/V_BDC_VENDORS_BANK_ACCOUNT_STG.js\n",
      "Generated file: ../../dataform/test_bill_dot_com/includes/staging/PAYMENTS/V_BDC_SENT_PAYS_STG.js\n",
      "Generated file: ../../dataform/test_bill_dot_com/includes/staging/PAYMENTS/V_BDC_RECEIVED_PAYS_STG.js\n",
      "Generated file: ../../dataform/test_bill_dot_com/includes/staging/DATES/V_BDC_DATES_STG.js\n",
      "Generated file: ../../dataform/test_bill_dot_com/includes/staging/INVOICES/V_BDC_INVOICES_PAY_STG.js\n",
      "Generated file: ../../dataform/test_bill_dot_com/includes/staging/INVOICES/V_BDC_INVOICES_HEADER_STG.js\n",
      "Generated file: ../../dataform/test_bill_dot_com/includes/staging/INVOICES/V_BDC_INVOICES_STG.js\n",
      "Generated file: ../../dataform/test_bill_dot_com/includes/staging/INVOICES/V_BDC_INVOICES_LINE_ITEM_STG.js\n",
      "Generated file: ../../dataform/test_bill_dot_com/includes/staging/CLASSES/V_BDC_ACTG_CLASSES_STG.js\n",
      "Generated file: ../../dataform/test_bill_dot_com/includes/staging/DEPARTMENTS/V_BDC_DEPARTMENTS_STG.js\n",
      "Generated file: ../../dataform/test_bill_dot_com/includes/staging/BILLS/V_BDC_BILLS_STG.js\n",
      "Generated file: ../../dataform/test_bill_dot_com/includes/staging/BILLS/V_BDC_BILLS_PAY_STG.js\n",
      "Generated file: ../../dataform/test_bill_dot_com/includes/staging/BILLS/V_BDC_BILLS_HEADER_STG.js\n",
      "Generated file: ../../dataform/test_bill_dot_com/includes/staging/BILLS/V_BDC_BILLS_LINE_ITEM_STG.js\n",
      "Generated file: ../../dataform/test_bill_dot_com/includes/staging/BILLS/V_BDC_BILLS_CREDIT_STG.js\n",
      "Generated file: ../../dataform/test_bill_dot_com/includes/staging/ACCOUNTS/V_BDC_CHART_OF_ACCOUNTS_STG.js\n",
      "Generated file: ../../dataform/test_bill_dot_com/includes/staging/LOCATIONS/V_BDC_LOCATIONS_STG.js\n",
      "Generated file: ../../dataform/test_bill_dot_com/includes/staging/JOBS/V_BDC_JOBS_STG.js\n",
      "Generated file: ../../dataform/test_bill_dot_com/includes/staging/ITEMS/V_BDC_ITEMS_STG.js\n",
      "Generated file: ../../dataform/test_bill_dot_com/includes/staging/EMPLOYEES/V_BDC_EMPLOYEES_STG.js\n",
      "Generated file: ../../dataform/test_bill_dot_com/includes/master/W_BDC_CUSTOMERS_D.js\n",
      "Generated file: ../../dataform/test_bill_dot_com/includes/master/W_BDC_VENDORS_D.js\n",
      "Generated file: ../../dataform/test_bill_dot_com/includes/master/W_BDC_DATES_D.js\n",
      "Generated file: ../../dataform/test_bill_dot_com/includes/master/W_BDC_JOBS_D.js\n",
      "Generated file: ../../dataform/test_bill_dot_com/includes/master/W_BDC_PAYMENTS_RECEIVED_F.js\n",
      "Generated file: ../../dataform/test_bill_dot_com/includes/master/W_BDC_TRANSACTIONS_F.js\n",
      "Generated file: ../../dataform/test_bill_dot_com/includes/master/W_BDC_INVOICES_F.js\n",
      "Generated file: ../../dataform/test_bill_dot_com/includes/master/W_BDC_ITEMS_D.js\n",
      "Generated file: ../../dataform/test_bill_dot_com/includes/master/W_BDC_ACTG_CLASSES_D.js\n",
      "Generated file: ../../dataform/test_bill_dot_com/includes/master/W_BDC_PAYMENTS_SENT_F.js\n",
      "Generated file: ../../dataform/test_bill_dot_com/includes/master/W_BDC_BILLS_F.js\n",
      "Generated file: ../../dataform/test_bill_dot_com/includes/master/W_BDC_DEPARTMENTS_D.js\n",
      "Generated file: ../../dataform/test_bill_dot_com/includes/master/W_BDC_LOCATIONS_D.js\n",
      "Generated file: ../../dataform/test_bill_dot_com/includes/master/W_BDC_EMPLOYEES_D.js\n",
      "Generated file: ../../dataform/test_bill_dot_com/includes/master/W_BDC_ACCOUNTS_D.js\n",
      "const H_INVOCATION_ID = require(\"./includes/H_INVOCATION_ID.js\");\n",
      "const V_BDC_CUSTOMERS_STG = require(\"./includes/staging/CUSTOMERS/V_BDC_CUSTOMERS_STG.js\");\n",
      "const V_BDC_CUSTOMERS_BANK_ACCOUNT_STG = require(\"./includes/staging/CUSTOMERS/V_BDC_CUSTOMERS_BANK_ACCOUNT_STG.js\");\n",
      "const V_BDC_VENDORS_BANK_ACCOUNT_STG = require(\"./includes/staging/VENDORS/V_BDC_VENDORS_BANK_ACCOUNT_STG.js\");\n",
      "const V_BDC_VENDORS_STG = require(\"./includes/staging/VENDORS/V_BDC_VENDORS_STG.js\");\n",
      "const V_BDC_SENT_PAYS_STG = require(\"./includes/staging/PAYMENTS/V_BDC_SENT_PAYS_STG.js\");\n",
      "const V_BDC_RECEIVED_PAYS_STG = require(\"./includes/staging/PAYMENTS/V_BDC_RECEIVED_PAYS_STG.js\");\n",
      "const V_BDC_DATES_STG = require(\"./includes/staging/DATES/V_BDC_DATES_STG.js\");\n",
      "const V_BDC_INVOICES_HEADER_STG = require(\"./includes/staging/INVOICES/V_BDC_INVOICES_HEADER_STG.js\");\n",
      "const V_BDC_INVOICES_LINE_ITEM_STG = require(\"./includes/staging/INVOICES/V_BDC_INVOICES_LINE_ITEM_STG.js\");\n",
      "const V_BDC_INVOICES_PAY_STG = require(\"./includes/staging/INVOICES/V_BDC_INVOICES_PAY_STG.js\");\n",
      "const V_BDC_INVOICES_STG = require(\"./includes/staging/INVOICES/V_BDC_INVOICES_STG.js\");\n",
      "const V_BDC_ACTG_CLASSES_STG = require(\"./includes/staging/CLASSES/V_BDC_ACTG_CLASSES_STG.js\");\n",
      "const V_BDC_DEPARTMENTS_STG = require(\"./includes/staging/DEPARTMENTS/V_BDC_DEPARTMENTS_STG.js\");\n",
      "const V_BDC_BILLS_LINE_ITEM_STG = require(\"./includes/staging/BILLS/V_BDC_BILLS_LINE_ITEM_STG.js\");\n",
      "const V_BDC_BILLS_HEADER_STG = require(\"./includes/staging/BILLS/V_BDC_BILLS_HEADER_STG.js\");\n",
      "const V_BDC_BILLS_CREDIT_STG = require(\"./includes/staging/BILLS/V_BDC_BILLS_CREDIT_STG.js\");\n",
      "const V_BDC_BILLS_STG = require(\"./includes/staging/BILLS/V_BDC_BILLS_STG.js\");\n",
      "const V_BDC_BILLS_PAY_STG = require(\"./includes/staging/BILLS/V_BDC_BILLS_PAY_STG.js\");\n",
      "const V_BDC_CHART_OF_ACCOUNTS_STG = require(\"./includes/staging/ACCOUNTS/V_BDC_CHART_OF_ACCOUNTS_STG.js\");\n",
      "const V_BDC_LOCATIONS_STG = require(\"./includes/staging/LOCATIONS/V_BDC_LOCATIONS_STG.js\");\n",
      "const V_BDC_JOBS_STG = require(\"./includes/staging/JOBS/V_BDC_JOBS_STG.js\");\n",
      "const V_BDC_ITEMS_STG = require(\"./includes/staging/ITEMS/V_BDC_ITEMS_STG.js\");\n",
      "const V_BDC_EMPLOYEES_STG = require(\"./includes/staging/EMPLOYEES/V_BDC_EMPLOYEES_STG.js\");\n",
      "const W_BDC_ITEMS_D = require(\"./includes/master/W_BDC_ITEMS_D.js\");\n",
      "const W_BDC_TRANSACTIONS_F = require(\"./includes/master/W_BDC_TRANSACTIONS_F.js\");\n",
      "const W_BDC_INVOICES_F = require(\"./includes/master/W_BDC_INVOICES_F.js\");\n",
      "const W_BDC_BILLS_F = require(\"./includes/master/W_BDC_BILLS_F.js\");\n",
      "const W_BDC_PAYMENTS_SENT_F = require(\"./includes/master/W_BDC_PAYMENTS_SENT_F.js\");\n",
      "const W_BDC_DATES_D = require(\"./includes/master/W_BDC_DATES_D.js\");\n",
      "const W_BDC_EMPLOYEES_D = require(\"./includes/master/W_BDC_EMPLOYEES_D.js\");\n",
      "const W_BDC_DEPARTMENTS_D = require(\"./includes/master/W_BDC_DEPARTMENTS_D.js\");\n",
      "const W_BDC_ACCOUNTS_D = require(\"./includes/master/W_BDC_ACCOUNTS_D.js\");\n",
      "const W_BDC_CUSTOMERS_D = require(\"./includes/master/W_BDC_CUSTOMERS_D.js\");\n",
      "const W_BDC_VENDORS_D = require(\"./includes/master/W_BDC_VENDORS_D.js\");\n",
      "const W_BDC_LOCATIONS_D = require(\"./includes/master/W_BDC_LOCATIONS_D.js\");\n",
      "const W_BDC_ACTG_CLASSES_D = require(\"./includes/master/W_BDC_ACTG_CLASSES_D.js\");\n",
      "const W_BDC_JOBS_D = require(\"./includes/master/W_BDC_JOBS_D.js\");\n",
      "const W_BDC_PAYMENTS_RECEIVED_F = require(\"./includes/master/W_BDC_PAYMENTS_RECEIVED_F.js\");\n",
      "\n",
      "\n",
      "module.exports = (params) => {\n",
      "  params = {\n",
      "    source_database: 'DEVELOPER_SANDBOX',\n",
      "    source_schema: 'DEMO_BILL_DOT_COM',\n",
      "    target_schema: 'BILL_COM',\n",
      "    ...params\n",
      "  };\n",
      "  const {\n",
      "    source_database,\n",
      "    source_schema,\n",
      "    target_schema\n",
      "} = params;\n",
      "\n",
      "const CUSTOMER = declare({\n",
      "  database: source_database,\n",
      "  schema: source_schema,\n",
      "  name: \"CUSTOMER\"\n",
      "});\n",
      "const CUSTOMER_BANK_ACCOUNT = declare({\n",
      "  database: source_database,\n",
      "  schema: source_schema,\n",
      "  name: \"CUSTOMER_BANK_ACCOUNT\"\n",
      "});\n",
      "const INVOICE = declare({\n",
      "  database: source_database,\n",
      "  schema: source_schema,\n",
      "  name: \"INVOICE\"\n",
      "});\n",
      "const INVOICE_LINE_ITEM = declare({\n",
      "  database: source_database,\n",
      "  schema: source_schema,\n",
      "  name: \"INVOICE_LINE_ITEM\"\n",
      "});\n",
      "const INVOICE_PAY = declare({\n",
      "  database: source_database,\n",
      "  schema: source_schema,\n",
      "  name: \"INVOICE_PAY\"\n",
      "});\n",
      "const RECEIVED_PAY = declare({\n",
      "  database: source_database,\n",
      "  schema: source_schema,\n",
      "  name: \"RECEIVED_PAY\"\n",
      "});\n",
      "const SENT_PAY = declare({\n",
      "  database: source_database,\n",
      "  schema: source_schema,\n",
      "  name: \"SENT_PAY\"\n",
      "});\n",
      "const BILL = declare({\n",
      "  database: source_database,\n",
      "  schema: source_schema,\n",
      "  name: \"BILL\"\n",
      "});\n",
      "const BILL_CREDIT = declare({\n",
      "  database: source_database,\n",
      "  schema: source_schema,\n",
      "  name: \"BILL_CREDIT\"\n",
      "});\n",
      "const BILL_LINE_ITEM = declare({\n",
      "  database: source_database,\n",
      "  schema: source_schema,\n",
      "  name: \"BILL_LINE_ITEM\"\n",
      "});\n",
      "const BILL_PAY = declare({\n",
      "  database: source_database,\n",
      "  schema: source_schema,\n",
      "  name: \"BILL_PAY\"\n",
      "});\n",
      "const ITEM = declare({\n",
      "  database: source_database,\n",
      "  schema: source_schema,\n",
      "  name: \"ITEM\"\n",
      "});\n",
      "const EMPLOYEE = declare({\n",
      "  database: source_database,\n",
      "  schema: source_schema,\n",
      "  name: \"EMPLOYEE\"\n",
      "});\n",
      "const DEPARTMENT = declare({\n",
      "  database: source_database,\n",
      "  schema: source_schema,\n",
      "  name: \"DEPARTMENT\"\n",
      "});\n",
      "const JOB = declare({\n",
      "  database: source_database,\n",
      "  schema: source_schema,\n",
      "  name: \"JOB\"\n",
      "});\n",
      "const LOCATION = declare({\n",
      "  database: source_database,\n",
      "  schema: source_schema,\n",
      "  name: \"LOCATION\"\n",
      "});\n",
      "const VENDOR = declare({\n",
      "  database: source_database,\n",
      "  schema: source_schema,\n",
      "  name: \"VENDOR\"\n",
      "});\n",
      "const VENDOR_BANK_ACCOUNT = declare({\n",
      "  database: source_database,\n",
      "  schema: source_schema,\n",
      "  name: \"VENDOR_BANK_ACCOUNT\"\n",
      "});\n",
      "const CHART_OF_ACCOUNT = declare({\n",
      "  database: source_database,\n",
      "  schema: source_schema,\n",
      "  name: \"CHART_OF_ACCOUNT\"\n",
      "});\n",
      "const ACTG_CLASS = declare({\n",
      "  database: source_database,\n",
      "  schema: source_schema,\n",
      "  name: \"ACTG_CLASS\"\n",
      "});\n",
      "  return {\n",
      "    CUSTOMER,\n",
      "    CUSTOMER_BANK_ACCOUNT,\n",
      "    INVOICE,\n",
      "    INVOICE_LINE_ITEM,\n",
      "    INVOICE_PAY,\n",
      "    RECEIVED_PAY,\n",
      "    SENT_PAY,\n",
      "    BILL,\n",
      "    BILL_CREDIT,\n",
      "    BILL_LINE_ITEM,\n",
      "    BILL_PAY,\n",
      "    ITEM,\n",
      "    EMPLOYEE,\n",
      "    DEPARTMENT,\n",
      "    JOB,\n",
      "    LOCATION,\n",
      "    VENDOR,\n",
      "    VENDOR_BANK_ACCOUNT,\n",
      "    CHART_OF_ACCOUNT,\n",
      "    ACTG_CLASS,\n",
      "    H_INVOCATION_ID: H_INVOCATION_ID(params),\n",
      "    V_BDC_CUSTOMERS_STG: V_BDC_CUSTOMERS_STG(params),\n",
      "    V_BDC_CUSTOMERS_BANK_ACCOUNT_STG: V_BDC_CUSTOMERS_BANK_ACCOUNT_STG(params),\n",
      "    V_BDC_VENDORS_BANK_ACCOUNT_STG: V_BDC_VENDORS_BANK_ACCOUNT_STG(params),\n",
      "    V_BDC_VENDORS_STG: V_BDC_VENDORS_STG(params),\n",
      "    V_BDC_SENT_PAYS_STG: V_BDC_SENT_PAYS_STG(params),\n",
      "    V_BDC_RECEIVED_PAYS_STG: V_BDC_RECEIVED_PAYS_STG(params),\n",
      "    V_BDC_DATES_STG: V_BDC_DATES_STG(params),\n",
      "    V_BDC_INVOICES_HEADER_STG: V_BDC_INVOICES_HEADER_STG(params),\n",
      "    V_BDC_INVOICES_LINE_ITEM_STG: V_BDC_INVOICES_LINE_ITEM_STG(params),\n",
      "    V_BDC_INVOICES_PAY_STG: V_BDC_INVOICES_PAY_STG(params),\n",
      "    V_BDC_INVOICES_STG: V_BDC_INVOICES_STG(params),\n",
      "    V_BDC_ACTG_CLASSES_STG: V_BDC_ACTG_CLASSES_STG(params),\n",
      "    V_BDC_DEPARTMENTS_STG: V_BDC_DEPARTMENTS_STG(params),\n",
      "    V_BDC_BILLS_LINE_ITEM_STG: V_BDC_BILLS_LINE_ITEM_STG(params),\n",
      "    V_BDC_BILLS_HEADER_STG: V_BDC_BILLS_HEADER_STG(params),\n",
      "    V_BDC_BILLS_CREDIT_STG: V_BDC_BILLS_CREDIT_STG(params),\n",
      "    V_BDC_BILLS_STG: V_BDC_BILLS_STG(params),\n",
      "    V_BDC_BILLS_PAY_STG: V_BDC_BILLS_PAY_STG(params),\n",
      "    V_BDC_CHART_OF_ACCOUNTS_STG: V_BDC_CHART_OF_ACCOUNTS_STG(params),\n",
      "    V_BDC_LOCATIONS_STG: V_BDC_LOCATIONS_STG(params),\n",
      "    V_BDC_JOBS_STG: V_BDC_JOBS_STG(params),\n",
      "    V_BDC_ITEMS_STG: V_BDC_ITEMS_STG(params),\n",
      "    V_BDC_EMPLOYEES_STG: V_BDC_EMPLOYEES_STG(params),\n",
      "    W_BDC_ITEMS_D: W_BDC_ITEMS_D(params),\n",
      "    W_BDC_TRANSACTIONS_F: W_BDC_TRANSACTIONS_F(params),\n",
      "    W_BDC_INVOICES_F: W_BDC_INVOICES_F(params),\n",
      "    W_BDC_BILLS_F: W_BDC_BILLS_F(params),\n",
      "    W_BDC_PAYMENTS_SENT_F: W_BDC_PAYMENTS_SENT_F(params),\n",
      "    W_BDC_DATES_D: W_BDC_DATES_D(params),\n",
      "    W_BDC_EMPLOYEES_D: W_BDC_EMPLOYEES_D(params),\n",
      "    W_BDC_DEPARTMENTS_D: W_BDC_DEPARTMENTS_D(params),\n",
      "    W_BDC_ACCOUNTS_D: W_BDC_ACCOUNTS_D(params),\n",
      "    W_BDC_CUSTOMERS_D: W_BDC_CUSTOMERS_D(params),\n",
      "    W_BDC_VENDORS_D: W_BDC_VENDORS_D(params),\n",
      "    W_BDC_LOCATIONS_D: W_BDC_LOCATIONS_D(params),\n",
      "    W_BDC_ACTG_CLASSES_D: W_BDC_ACTG_CLASSES_D(params),\n",
      "    W_BDC_JOBS_D: W_BDC_JOBS_D(params),\n",
      "    W_BDC_PAYMENTS_RECEIVED_F: W_BDC_PAYMENTS_RECEIVED_F(params),\n",
      "\n",
      "  }\n",
      "}\n",
      "Created index.js file\n",
      "Created example.js file\n"
     ]
    }
   ],
   "source": [
    "dbt_dataform_converter(dataform_root_path,dbt_models_file_path,dbt_snapshots_file_path,dataform_credentials_file_path,dataform_output_models_path,dataform_output_includes_path,current_directory,conversion_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dbt_dataform_converter(dataform_root_path,dbt_models_file_path,dbt_snapshots_file_path,dataform_credentials_file_path,dataform_output_models_path,dataform_output_includes_path,current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
