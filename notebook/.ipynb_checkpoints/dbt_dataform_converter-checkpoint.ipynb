{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil #directory controls\n",
    "import json\n",
    "import yaml\n",
    "import shutil\n",
    "import fnmatch\n",
    "import errno\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set path variables\n",
    "\n",
    "dbt_sources_file_path = \"../../dbt/square_dbt/models/sources/\" #dbt-sources-path\n",
    "dbt_models_file_pathdbt_models_file_path = \"../../dbt/square_dbt/models/\" #dbt-sources-path\n",
    "dbt_snapshots_file_path = \"../../dbt/square_dbt/snapshots/\" #dbt-sources-path\n",
    "\n",
    "dataform_output_sources_path = \"../_dataform_output/definitions/sources\"\n",
    "dataform_output_models_path = \"../_dataform_output/definitions\"\n",
    "\n",
    "dataform_root_path = \"../_dataform_output\"\n",
    "target_schema = 'DATAFORM_SQUARE'\n",
    "dlh_timestamp_field = '\"MD_ELT_UPDATED_DTS\"'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find files on directories and subdirectories\n",
    "def find_files(directory, pattern):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for basename in files:\n",
    "            if fnmatch.fnmatch(basename, pattern):\n",
    "                filename = os.path.join(root, basename)\n",
    "                yield filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YAML loader\n",
    "def read_yaml_file(filename):\n",
    "    with open(filename, 'r') as stream:\n",
    "        try:\n",
    "            yaml_dict = yaml.load(stream, Loader=yaml.FullLoader)\n",
    "            return yaml_dict\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "\n",
    "#YAML -> SQLx conerter\n",
    "def create_sqlx_json_source_file(source_file_path):\n",
    "\n",
    "#creating directory\n",
    "    isExist = os.path.exists(dataform_output_sources_path)\n",
    "    if not isExist:\n",
    "        os.makedirs(dataform_output_sources_path)\n",
    "        \n",
    "    for file in source_file_path:\n",
    "        #reading all YAML files on source directory\n",
    "        dic = read_yaml_file(file)\n",
    "        #getting only sources entity from yaml file\n",
    "        sources = dic[\"sources\"]\n",
    "        \n",
    "        for source in sources:\n",
    "            #declaring variables for each element of dictionary that will be used. tables = list, schema and database = str\n",
    "            tables = source[\"tables\"]            \n",
    "            database = source[\"database\"]\n",
    "            schema = source[\"schema\"]\n",
    "            \n",
    "            #iterating through all tables for each schema\n",
    "            for tables in tables:\n",
    "                    #getting tables from list\n",
    "                    tables= tables['name']                                \n",
    "                    #creating json .sqlx structure\n",
    "                    jmodel = \"{\\n\"\n",
    "                    jmodel += f'\"type\": \"declaration\",\\n'                \n",
    "                    jmodel += f'\"database\": \"{database}\",\\n'\n",
    "                    jmodel += f'\"schema\": \"{schema}\",\\n'\n",
    "                    jmodel += f'\"name\": \"{tables}\"'\n",
    "                    jmodel += \"\\n}\" #close out JSON file EOF\n",
    "\n",
    "                    #parsing json and replacing double quotes for single quotes on keys\n",
    "                    parsed = json.dumps(json.loads(jmodel), sort_keys=False, indent=2).replace('\"type\":','type:').replace('\"database\":','database:').replace('\"schema\":','schema:').replace('\"name\":','name:')\n",
    "\n",
    "                    #creating .sqlx files\n",
    "                    #write out to file in the appropriate location defined in variables\n",
    "                    with open(f'{dataform_output_sources_path}/{schema}_{tables}.sqlx', \"w\") as jmodel_file:\n",
    "                        #jmodel_file.write( script_header_template )\n",
    "                        jmodel_file.write(\"config \"+parsed)\n",
    "                        jmodel_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sqlx_models_files(model_files_path):\n",
    "    for filename in model_files_path:  \n",
    "        single_file_name = os.path.basename(filename)\n",
    "        destination_path = os.path.dirname(filename)\n",
    "        destination_path = dataform_output_models_path+(destination_path.split('models')[1])+'/'\n",
    "        destination_full_path = (destination_path+(os.path.basename(filename)).replace(\"sql\",\"sqlx\"))\n",
    "        \n",
    "     #######copying files to new directory##############   \n",
    "        try:\n",
    "            #copy files if directory already exists\n",
    "            \n",
    "            shutil.copyfile(filename,destination_full_path)\n",
    "        except IOError as e:\n",
    "            # ENOENT(2): file does not exist, raised also on missing dest parent dir\n",
    "            if e.errno != errno.ENOENT:\n",
    "                raise\n",
    "            # create directory if not exists\n",
    "            \n",
    "            os.makedirs(destination_path)\n",
    "            # copy source files to new directory\n",
    "            \n",
    "            shutil.copyfile(filename,destination_full_path)\n",
    "    #######end copying files to new directory##############\n",
    "\n",
    "    #######reading file and replacing model syntax differences########\n",
    "        # Read in the file\n",
    "        with open(destination_full_path, 'r') as file :\n",
    "            filedata = file.read()\n",
    "\n",
    "            #getting header data to be replaced\n",
    "            header=filedata[filedata.find(\"{{\"):filedata.find(\"}}\")+2]\n",
    "            header_old=header\n",
    "\n",
    "            header_replace_dict = {\"{{\":\"\", \"}}\":\"}\", \"materialized\":\"type\",\"=\":\":\",\"(\":\"{\",\")\":\"\",\"'\":'\"',\"unique_key\":'uniqueKey'}\n",
    "\n",
    "            #iterate through dictionary keys\n",
    "            for key in header_replace_dict.keys():\n",
    "                #replacing all headers\n",
    "                header = header.replace(key, header_replace_dict[key])         \n",
    "                \n",
    "\n",
    "        #writing file with replaced header and models references\n",
    "        with open(destination_full_path, 'w') as file:\n",
    "            #replacing files content\n",
    "            new_file = filedata.replace(header_old,header)\n",
    "            #removing transient parameter from header\n",
    "            new_file = re.sub(r'(transient)[ ]{0,9}[\\:][ ]{0,9}(true|false|True|False|TRUE|FALSE)','',new_file)\n",
    "            #changing {{ref to ${ref\n",
    "            new_file = re.sub(r\"[{][{][ ]{0,6}(ref|REF|Ref|SOURCE|source|Source)[ ]{0,9}[(][ ]{0,9}[']?\",'${ref(\"', new_file)\n",
    "            \n",
    "            #including just 1 space before config header\n",
    "            new_file = re.sub(r'(config)[ ]{0,40}[{]{0,40}','config {',new_file)\n",
    "            #closing ) with \") after sources and refs\n",
    "            new_file = re.sub(r\"(['][)][ ]{0,9}[}])\",'\")',new_file)\n",
    "            #removing alone commmas on start of config block\n",
    "            new_file = re.sub(r\"({)\\n[ ]{0,100}[,]\",'{',new_file)\n",
    "            #changing syntax reference to macros from {{ to ${\n",
    "            new_file = re.sub(r\"(?<!\\')({{)[ ]{0,20}[.]{0,20}\",'${',new_file)\n",
    "            new_file = re.sub(r\"([\\$].+)[ ]{0,9}(?<=\\()(')\",r'\\1\"',new_file)\n",
    "            #changing ' to \" on sources references\n",
    "            new_file = re.sub(r\"([\\$].+)((?<=[0-9A-Za-z])[ ]{0,9}[_'][.]{0,10}[ ]{0,20}[,][.]{0,20}[ ]{0,20}[.]{0,20}['])\",r'\\1\",\"',new_file)\n",
    "            #changing invocation_id to snowflake uuid_string\n",
    "            new_file = re.sub(r\"('{{invocation_id}}'|{{invocation_id}})\",'uuid_string()',new_file)\n",
    "            #changing incremental loads macro\n",
    "            new_file = re.sub(r\"({[ ]{0,10}%[ ]{0,10}if[ ]{0,10}is_incremental.*)((\\s*)(.*)\\s*)({%[ ]{0,10}endif..*})\",r'${ when(incremental(), \\n`\\4`) }',new_file)\n",
    "            new_file = re.sub(r\"((?:\\${[ ]{0,10}when.*(\\s.*)))(\\${this\\}\\})\",r'\\1${self()}',new_file)\n",
    "            \n",
    "            \n",
    "\n",
    "            try:\n",
    "                file.write(new_file)                \n",
    "                print('Generated file: '+destination_full_path)\n",
    "            except IOError as e:\n",
    "                print (\"I/O error({0}): {1}\".format(e.errno, e.strerror))\n",
    "    #######end reading file and replacing model syntax differences########\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sqlx_snapshot_files(snapshots_file_path):\n",
    "    for filename in snapshots_file_path:  \n",
    "        single_file_name = os.path.basename(filename)\n",
    "        destination_path = os.path.dirname(filename)\n",
    "        destination_path = dataform_output_models_path+'/snapshots'+(destination_path.split('snapshots')[1])+'/'\n",
    "        destination_full_path = (destination_path+(os.path.basename(filename)).replace(\".sql\",\".js\"))\n",
    "        \n",
    "     #######copying files to new directory##############   \n",
    "        try:\n",
    "            #copy files if directory already exists\n",
    "            \n",
    "            shutil.copyfile(filename,destination_full_path)\n",
    "        except IOError as e:\n",
    "            # ENOENT(2): file does not exist, raised also on missing dest parent dir\n",
    "            if e.errno != errno.ENOENT:\n",
    "                raise\n",
    "            # create directory if not exists\n",
    "            \n",
    "            os.makedirs(destination_path)\n",
    "            # copy source files to new directory\n",
    "            \n",
    "            shutil.copyfile(filename,destination_full_path)\n",
    "    #######end copying files to new directory##############\n",
    "    \n",
    "     \n",
    "                    \n",
    "        with open(destination_full_path, 'r') as file :\n",
    "            filedata = file.read()   \n",
    "            table = re.search(r\"({{)[ ]{0,9}(ref).*((?<=').*(?='))\",filedata).group(3)\n",
    "            file_name = single_file_name.replace('.sql','')\n",
    "            \n",
    "            #print(table)\n",
    "            filedata = re.sub(r'({%)[ ]{0,10}(endsnapshot|snapshot).*(})','',filedata)\n",
    "            filedata = re.sub(r\"(check_cols.*[\\]]{1,1}|check_cols.*[\\\"]{1,1})\",'timestamp: '+dlh_timestamp_field,filedata)            \n",
    "            filedata = re.sub(r'(target_database.*,|target_schema.*,|strategy.*|invalidate_hard_deletes.*|check_cols.*)','',filedata)\n",
    "            filedata=filedata[filedata.find(\"{\"):filedata.find(\"}\")+2]\n",
    "            \n",
    "            \n",
    "            filedata = filedata.replace(')', 'source: {\\n schema: \"{{schema}}\",\\n name: \"{{table}}\",\\n}, \\n});')\n",
    "            filedata = filedata.replace('{{schema}}',target_schema).replace('{{table}}',table)\n",
    "            replace_dict = {\"updated_at\":\"timestamp\",\"unique_key\":\"  uniqueKey\", \"config\":'scd(\"{{source_data_scd}}\", ',\"'\":'\"',\"{{\":\"\",\"}}\":\"\",\"=\":\": \"}\n",
    "\n",
    "            #iterate through dictionary keys\n",
    "            for key in replace_dict.keys():\n",
    "                #replacing all headers\n",
    "                filedata = filedata.replace(key, replace_dict[key]).replace('{{source_data_scd}}',file_name)\n",
    "                filedata = re.sub(r'(scd\\(.*)(\\()',r'\\1{',filedata)\n",
    "\n",
    "                \n",
    "        #writing file with replaced header and models references\n",
    "        with open(destination_full_path, 'w') as file:\n",
    "            #replacing files content\n",
    "            new_file = filedata\n",
    "            new_file = re.sub(r'(strategy)[ ]{0,9}[\\:][ ]{0,9}.*(,)','',new_file)            \n",
    "            new_file = re.sub(r\"[ ]{2,999}\",'',new_file)\n",
    "            new_file = re.sub(r\"^\\s*$\",'',new_file,re.MULTILINE)\n",
    "            new_file = re.sub(r'\\n\\s*\\n','\\n',new_file,re.MULTILINE)            \n",
    "            new_file = new_file.replace('uniqueKey:','  uniqueKey:').replace('timestamp:','  timestamp:').replace('source:','  source:').replace('schema:','   schema:').replace('name:','   name:').replace('},','  },')\n",
    "            #print(new_file)\n",
    "            try:\n",
    "                file.write('const scd = require(\"dataform-scd\");\\n\\n')\n",
    "                file.write(new_file)\n",
    "                print('Generated file: '+destination_full_path)\n",
    "            except IOError as e:\n",
    "                print (\"I/O error({0}): {1}\".format(e.errno, e.strerror))\n",
    "    #######end reading file and replacing model syntax differences########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataform_install_configuration(dataform_root_path):\n",
    "    \n",
    "    try:\n",
    "        shutil.rmtree(dataform_root_path)\n",
    "    except OSError as err:\n",
    "        print(err)\n",
    "\n",
    "    \n",
    "    os.system(\"dataform init snowflake \"+dataform_root_path)\n",
    "    shutil.copy(\"../.df-credentials.json\",dataform_root_path)\n",
    "\n",
    "    \n",
    "    packages_file = dataform_root_path+ '/' +'package.json'\n",
    "        \n",
    "    with open(packages_file, 'r') as file:\n",
    "        packages_file_data = file.read()\n",
    "        \n",
    "    #with open(packages_file, 'w') as file:        \n",
    "        #packages_file_data = packages_file_data.replace('    }','    \"    dataform-scd\": \"git+https://github.com/dataform-co/dataform-scd.git#0.1\"\\n    }')\n",
    "        #try:                \n",
    "        #    file.write(packages_file_data)                \n",
    "        #except IOError as e:\n",
    "        #    print (\"I/O error({0}): {1}\".format(e.errno, e.strerror))\n",
    "                \n",
    "    #adding scd package to package.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataform_assertions_documentation(dbt_models_file_path,schema_files):\n",
    "\n",
    "    for file in schema_files:\n",
    "        #reading all YAML files on source directory\n",
    "        dic = read_yaml_file(file)\n",
    "        #getting only sources entity from yaml file\n",
    "        models_dir = dic[\"models\"]\n",
    "        dictionary={}\n",
    "        #print(models_dir)\n",
    "        unique_list=[]\n",
    "        not_null_list=[]\n",
    "        not_null = {} \n",
    "        table_descriptions = {} \n",
    "        description={}\n",
    "        unique = {}  \n",
    "        tables_list=[]\n",
    "        description_list=[]\n",
    "        table_description_list=[]\n",
    "        for model in models_dir:\n",
    "            #declaring variables for each element of dictionary that will be used. tables = list, schema and database = str        \n",
    "            #print(model[\"name\"])\n",
    "\n",
    "            columns = (model[\"columns\"]) \n",
    "            tables = (model[\"name\"])\n",
    "\n",
    "\n",
    "            tables_list.append(tables)\n",
    "\n",
    "\n",
    "            if 'description' in model: \n",
    "                descriptions = (model[\"description\"])\n",
    "                table_descriptions = {descriptions:tables}\n",
    "                table_description_list.append(table_descriptions.copy())\n",
    "\n",
    "\n",
    "            #print(dictionary)\n",
    "            for column in columns:\n",
    "                tests= (column.get('tests'))\n",
    "\n",
    "                column_get = column.get('name')           \n",
    "                descriptions= (column.get('description'))\n",
    "\n",
    "                #print(tests)\n",
    "\n",
    "                if 'unique' in tests:\n",
    "                    unique = {column_get:tables}                                                        \n",
    "                    unique_list.append(unique.copy())\n",
    "\n",
    "                if 'not_null' in tests:\n",
    "                    not_null = {column_get:tables}\n",
    "                    not_null_list.append(not_null.copy())\n",
    "\n",
    "                if 'description'in column:\n",
    "                    description = {'|    '+column_get+': '+\"'\"+descriptions+\"'\":tables}\n",
    "                    description_list.append(description.copy())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #print(not_null_list)\n",
    "\n",
    "        keyfunc = lambda d: next(iter(d.values()))   \n",
    "\n",
    "\n",
    "        not_null_dict={k: [x for d in g for x in d] \n",
    "            for k, g in itertools.groupby(sorted(not_null_list, key=keyfunc), key=keyfunc)}\n",
    "\n",
    "        unique_dict={k: [x for d in g for x in d] \n",
    "            for k, g in itertools.groupby(sorted(unique_list, key=keyfunc), key=keyfunc)}\n",
    "\n",
    "        description_dict={k: [x for d in g for x in d] \n",
    "            for k, g in itertools.groupby(sorted(description_list, key=keyfunc), key=keyfunc)}\n",
    "\n",
    "        table_description_dict={k: [x for d in g for x in d] \n",
    "                for k, g in itertools.groupby(sorted(table_description_list, key=keyfunc), key=keyfunc)}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # list of all .sql files in a directory\n",
    "        dataform_files = find_files(dataform_output_models_path, '*.sqlx')\n",
    "\n",
    "\n",
    "        files_list =[]\n",
    "        file_name_list =[]\n",
    "        for file in dataform_files:\n",
    "\n",
    "            if os.path.basename(file).replace('.sqlx','') in tables_list:\n",
    "                file_name = os.path.basename(file)\n",
    "\n",
    "                destination_path = os.path.dirname(file)\n",
    "\n",
    "                full_path = destination_path+'/'+file_name     \n",
    "\n",
    "\n",
    "                files_list.append(file_name.replace('.sqlx',''))\n",
    "\n",
    "\n",
    "\n",
    "                with open(full_path, 'r') as file :\n",
    "\n",
    "                    file_name = file_name.replace('.sqlx','')\n",
    "\n",
    "                    if file_name in table_description_dict.keys():\n",
    "                        jmodel_tables_description='  description: '+str(table_description_dict[file_name]).replace(\"'\",'\"').replace('[','').replace(']','')+',\\n'\n",
    "\n",
    "\n",
    "\n",
    "                    filedata = file.read()\n",
    "                    if file_name in unique_dict.keys() or file_name in unique_dict.keys():\n",
    "                        jmodel='  assertions: {\\n'\n",
    "\n",
    "\n",
    "                        #print(file)\n",
    "                        if file_name in unique_dict.keys():                    \n",
    "                            jmodel+='    uniqueKey: '+str(unique_dict[file_name]).replace(\"'\",'\"')+',\\n'\n",
    "\n",
    "                        if file_name.replace('.sqlx','') in not_null_dict.keys():\n",
    "                            jmodel+='    nonNull: '+str(not_null_dict[file_name]).replace(\"'\",'\"')+'\\n'\n",
    "\n",
    "                        jmodel+='\\n  }\\n}'\n",
    "                        #print(filedata)\n",
    "\n",
    "\n",
    "\n",
    "                    if file_name.replace('.sqlx','') in description_dict.keys():         \n",
    "                        jmodel_description = '  columns: {\\n'\n",
    "                        jmodel_description += str(description_dict[file_name]).replace('\"','').replace(\"'\",'\"').replace('[','').replace(']','').replace('|','\\n')\n",
    "                        jmodel_description += '\\n  }\\n'\n",
    "\n",
    "\n",
    "                    header=filedata[filedata.find(\"{\"):filedata.find(\"}\")+2]                \n",
    "                    header_old=header\n",
    "                    header = header.replace('}','')+jmodel_tables_description+jmodel_description+jmodel\n",
    "\n",
    "\n",
    "                    with open(f'{full_path}', \"w\") as jmodel_file:\n",
    "\n",
    "                            jmodel_file.write(filedata.replace(header_old,header))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all .sql files in a directory\n",
    "model_files = find_files(dbt_models_file_path, '*.sql')\n",
    "# list of all .yaml files in a directory\n",
    "source_files = find_files(dbt_sources_file_path, '*.yml')\n",
    "# list of all .sql files in a directory\n",
    "snapshot_files = find_files(dbt_snapshots_file_path, '*.sql')\n",
    "# list of test and documentation files\n",
    "schema_files = find_files(dbt_models_file_path, 'schema.yml')\n",
    "\n",
    "def dbt_dataform_converter(dataform_root_path,source_files,model_files,snapshot_files,schema_files):\n",
    "    \n",
    "    \n",
    "    #installing dataform\n",
    "    dataform_install_configuration(dataform_root_path)    \n",
    "    #creating source\n",
    "    create_sqlx_json_source_file(source_files)\n",
    "    #creating models\n",
    "    create_sqlx_models_files(model_files)\n",
    "    #creating snapshot files\n",
    "    create_sqlx_snapshot_files(snapshot_files)\n",
    "    #addind test to models\n",
    "    dataform_assertions_documentation(dbt_models_file_path,schema_files)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing project files...\n",
      "\n",
      "\u001b[32mDirectories successfully created:\u001b[0m\n",
      "  /Users/guilhermealcantara/OneDrive/brf consulting/dbt - dataform converter/_dataform_output\n",
      "  /Users/guilhermealcantara/OneDrive/brf consulting/dbt - dataform converter/_dataform_output/definitions\n",
      "  /Users/guilhermealcantara/OneDrive/brf consulting/dbt - dataform converter/_dataform_output/includes\n",
      "\u001b[32mFiles successfully written:\u001b[0m\n",
      "  /Users/guilhermealcantara/OneDrive/brf consulting/dbt - dataform converter/_dataform_output/dataform.json\n",
      "  /Users/guilhermealcantara/OneDrive/brf consulting/dbt - dataform converter/_dataform_output/package.json\n",
      "  /Users/guilhermealcantara/OneDrive/brf consulting/dbt - dataform converter/_dataform_output/.gitignore\n",
      "\u001b[32mNPM packages successfully installed.\u001b[0m\n",
      "Generated file: ../_dataform_output/definitions/staging/CUSTOMERS/V_CUSTOMER_STG.sqlx\n",
      "Generated file: ../_dataform_output/definitions/staging/LOCATION/V_MERCHANT_LOCATION_STG.sqlx\n",
      "Generated file: ../_dataform_output/definitions/staging/PAYMENT/V_PAYMENT_STG.sqlx\n",
      "Generated file: ../_dataform_output/definitions/staging/CATALOG/V_CATALOG_ITEM_MODIFIER_STG.sqlx\n",
      "Generated file: ../_dataform_output/definitions/staging/CATALOG/V_CATALOG_TAX_STG.sqlx\n",
      "Generated file: ../_dataform_output/definitions/staging/CATALOG/V_CATALOG_CATEGORY_STG.sqlx\n",
      "Generated file: ../_dataform_output/definitions/staging/CATALOG/V_CATALOG_DISCOUNT_STG.sqlx\n",
      "Generated file: ../_dataform_output/definitions/staging/CATALOG/V_CATALOG_ITEM_VARIATION_STG.sqlx\n",
      "Generated file: ../_dataform_output/definitions/staging/DATE/V_DATE_STG.sqlx\n",
      "Generated file: ../_dataform_output/definitions/staging/ORDERS/V_ORDER_LINE_ITEM_STG.sqlx\n",
      "Generated file: ../_dataform_output/definitions/staging/ORDERS/V_ORDER_HEADER_STG.sqlx\n",
      "Generated file: ../_dataform_output/definitions/staging/ORDERS/V_ORDER_LINE_ITEM_MODIFIER_STG.sqlx\n",
      "Generated file: ../_dataform_output/definitions/staging/CURRENCY/V_CURRENCY_STG.sqlx\n",
      "Generated file: ../_dataform_output/definitions/master/W_ORDERS_F.sqlx\n",
      "Generated file: ../_dataform_output/definitions/master/W_DATE_D.sqlx\n",
      "Generated file: ../_dataform_output/definitions/master/W_MERCHANT_LOCATION_D.sqlx\n",
      "Generated file: ../_dataform_output/definitions/master/W_PAYMENTS_F.sqlx\n",
      "Generated file: ../_dataform_output/definitions/master/W_CUSTOMERS_D.sqlx\n",
      "Generated file: ../_dataform_output/definitions/master/W_CURRENCY_D.sqlx\n",
      "Generated file: ../_dataform_output/definitions/master/W_CATALOG_ITEM_D.sqlx\n",
      "Generated file: ../_dataform_output/definitions/snapshots/W_CUSTOMERS_SNAPSHOT_D.js\n"
     ]
    }
   ],
   "source": [
    "dbt_dataform_converter(dataform_root_path,source_files,model_files,snapshot_files,schema_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
