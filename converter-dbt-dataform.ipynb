{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil #directory controls\n",
    "import glob\n",
    "import json\n",
    "import sqlparse #new as of 20200629\n",
    "from jsonpath_ng import jsonpath, parse\n",
    "import datetime #new as of 20210411CS\n",
    "import yaml\n",
    "import fnmatch\n",
    "import shutil\n",
    "import errno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guilherme Alcantara\\OneDrive\\brf consulting\\dbt - dataform converter\n"
     ]
    }
   ],
   "source": [
    "#set path variables\n",
    "print(os.path.abspath(''))\n",
    "\n",
    "dbt_sources_file_path = \"../dbt/mcleod_dbt/models/sources/\" #dbt-sources-path\n",
    "dbt_models_file_path = \"../dbt/mcleod_dbt/models/\" #dbt-sources-path\n",
    "dbt_snapshots_file_path = \"../dbt/mcleod_dbt/snapshots/\" #dbt-sources-path\n",
    "\n",
    "dataform_output_sources_path = \"_dataform_output/sources\"\n",
    "dataform_output_models_path = \"_dataform_output\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#20210411CS - added to use in the template creation dates with auto generated\n",
    "dateYYYYMMDD = datetime.datetime.now().strftime(\"%Y%m%d\") #.strftime(\"%Y%m%d-%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating directory\n",
    "isExist = os.path.exists(dataform_output_sources_path)\n",
    "if not isExist:\n",
    "    os.makedirs(dataform_output_sources_path)\n",
    "else:\n",
    "    #delete all files from target\n",
    "    shutil.rmtree(dataform_output_sources_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find files on directories and subdirectories\n",
    "def find_files(directory, pattern):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for basename in files:\n",
    "            if fnmatch.fnmatch(basename, pattern):\n",
    "                filename = os.path.join(root, basename)\n",
    "                yield filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all .yaml files in a directory\n",
    "source_files = find_files(dbt_sources_file_path, '*.yml')\n",
    "\n",
    "#YAML loader\n",
    "def read_yaml_file(filename):\n",
    "    with open(filename, 'r') as stream:\n",
    "        try:\n",
    "            yaml_dict = yaml.load(stream, Loader=yaml.FullLoader)\n",
    "            return yaml_dict\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "\n",
    "#YAML -> SQLx conerter\n",
    "def create_sqlx_json_source_file(source_file_path):\n",
    "\n",
    "\n",
    "    for file in source_file_path:\n",
    "        #reading all YAML files on source directory\n",
    "        dic = read_yaml_file(file)\n",
    "        #getting only sources entity from yaml file\n",
    "        sources = dic[\"sources\"]\n",
    "        \n",
    "        for source in sources:\n",
    "            #declaring variables for each element of dictionary that will be used. tables = list, schema and database = str\n",
    "            tables = source[\"tables\"]\n",
    "            database = source[\"database\"]\n",
    "            schema = source[\"schema\"]\n",
    "            #iterating through all tables for each schema\n",
    "            for tables in tables:\n",
    "                    #getting tables from list\n",
    "                    tables= tables['name']\n",
    "                    #creating json .sqlx structure\n",
    "                    jmodel = \"{\\n\"\n",
    "                    jmodel += f'\"type\": \"declaration\",\\n'                \n",
    "                    jmodel += f'\"database\": \"{database}\",\\n'\n",
    "                    jmodel += f'\"schema\": \"{schema}\",\\n'\n",
    "                    jmodel += f'\"name\": \"{tables}\"'\n",
    "                    jmodel += \"\\n}\" #close out JSON file EOF\n",
    "\n",
    "                    #parsing json and replacing double quotes for single quotes on keys\n",
    "                    parsed = json.dumps(json.loads(jmodel), sort_keys=False, indent=2).replace('\"type\":','type:').replace('\"database\":','database:').replace('\"schema\":','schema:').replace('\"name\":','name:')\n",
    "\n",
    "                    #creating .sqlx files\n",
    "                    #write out to file in the appropriate location defined in variables\n",
    "                    with open(f'{dataform_output_sources_path}/{schema}_{tables}.sqlx', \"w\") as jmodel_file:\n",
    "                        #jmodel_file.write( script_header_template )\n",
    "                        jmodel_file.write(\"config \"+parsed)\n",
    "                        jmodel_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_sqlx_json_source_file(source_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sqlx_models_files(model_files_path):\n",
    "    for filename in model_files_path:  \n",
    "        single_file_name = os.path.basename(filename)\n",
    "        destination_path = (dataform_output_models_path+(filename[filename.find('models')+len('models'):filename.rfind('\\\\')]))+'/'\n",
    "        destination_full_path = (destination_path+(os.path.basename(filename)).replace(\"sql\",\"sqlx\"))\n",
    "        \n",
    "        \n",
    "        #print(destination_path)\n",
    "        #print(single_file_name)\n",
    "        #print(destination_full_path)\n",
    "     #######copying files to new directory##############   \n",
    "        try:\n",
    "            #copy files if directory already exists\n",
    "            \n",
    "            shutil.copyfile(filename,destination_full_path)\n",
    "        except IOError as e:\n",
    "            # ENOENT(2): file does not exist, raised also on missing dest parent dir\n",
    "            if e.errno != errno.ENOENT:\n",
    "                raise\n",
    "            # create directory if not exists\n",
    "            \n",
    "            os.makedirs(destination_path)\n",
    "            # copy source files to new directory\n",
    "            \n",
    "            shutil.copyfile(filename,destination_full_path)\n",
    "    #######end copying files to new directory##############\n",
    "\n",
    "    #######reading file and replacing model syntax differences########\n",
    "        # Read in the file\n",
    "        with open(destination_full_path, 'r') as file :\n",
    "            filedata = file.read()\n",
    "\n",
    "            #getting header data to be replaced\n",
    "            header=filedata[filedata.find(\"{{\"):filedata.find(\"}}\")+2]\n",
    "            header_old=header\n",
    "\n",
    "\n",
    "            header_replace_dict = {\"{{\":\"\", \"}}\":\"}\", \"materialized\":\"type\",\"=\":\":\",\"(\":\"{\",\")\":\"\",\"'\":'\"'}\n",
    "\n",
    "            #iterate through dictionary keys\n",
    "            for key in header_replace_dict.keys():\n",
    "                #replacing all headers\n",
    "                header = header.replace(key, header_replace_dict[key])         \n",
    "\n",
    "\n",
    "        #writing file with replaced header and models references\n",
    "        with open(destination_full_path, 'w') as file:\n",
    "            new_file = filedata.replace(header_old,header)\n",
    "            new_file = re.sub(r\"[{][{][ ]{0,6}(ref|REF|Ref|SOURCE|source|Source)[ ]{0,9}[(][ ]{0,9}[']?\",'${ref(\"', new_file)            \n",
    "            new_file = re.sub(r\"([ ]{0,9}[_'][.]{0,10}[ ]{0,20}(,)[.]{0,20}[ ]{0,20}[.]{0,20}['])\",'_',new_file)\n",
    "            new_file = re.sub(r\"(['][)][ ]{0,9}[}])\",'\")',new_file)\n",
    "\n",
    "            try:\n",
    "                file.write(new_file)                \n",
    "                print('Generated file: '+destination_full_path)\n",
    "            except OSError as error : \n",
    "                print(error) \n",
    "    #######end reading file and replacing model syntax differences########\n",
    "        \n",
    "           \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated file: _dataform_output/master/a_data_count_validations.sqlx\n",
      "Generated file: _dataform_output/master/w_call_record_d.sqlx\n",
      "Generated file: _dataform_output/master/w_destination_stop_d.sqlx\n",
      "Generated file: _dataform_output/master/w_driver_f.sqlx\n",
      "Generated file: _dataform_output/master/w_edi_orders_d.sqlx\n",
      "Generated file: _dataform_output/master/w_equipment_lkp.sqlx\n",
      "Generated file: _dataform_output/master/w_movement_d.sqlx\n",
      "Generated file: _dataform_output/master/w_movement_f.sqlx\n",
      "Generated file: _dataform_output/master/w_open_item_d.sqlx\n",
      "Generated file: _dataform_output/master/w_open_item_lkp.sqlx\n",
      "Generated file: _dataform_output/master/w_order_detail_ext_f.sqlx\n",
      "Generated file: _dataform_output/master/w_order_detail_f.sqlx\n",
      "Generated file: _dataform_output/master/w_origin_stop_d.sqlx\n",
      "Generated file: _dataform_output/master/w_other_charge_lkp.sqlx\n",
      "Generated file: _dataform_output/master/w_segment_d.sqlx\n",
      "Generated file: _dataform_output/master/w_tractor_movement_d.sqlx\n",
      "Generated file: _dataform_output/master/w_weather_conditions_d.sqlx\n",
      "Generated file: _dataform_output/staging/w_company_d_stg.sqlx\n",
      "Generated file: _dataform_output/staging/w_customer_d_stg.sqlx\n",
      "Generated file: _dataform_output/staging/w_driver_d_stg.sqlx\n",
      "Generated file: _dataform_output/staging/w_location_d_stg.sqlx\n",
      "Generated file: _dataform_output/staging/w_orders_d_stg.sqlx\n",
      "Generated file: _dataform_output/staging/w_tractor_d_stg.sqlx\n",
      "Generated file: _dataform_output/staging/w_trailer_d_stg.sqlx\n"
     ]
    }
   ],
   "source": [
    "# list of all .yaml files in a directory\n",
    "model_files = find_files(dbt_models_file_path, '*.sql')\n",
    "\n",
    "create_sqlx_models_files(model_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
